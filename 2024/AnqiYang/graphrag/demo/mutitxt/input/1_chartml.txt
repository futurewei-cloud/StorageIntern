On Pre-training of Multimodal Language Models
Customized for Chart Understanding

Wan-Cyuan Fan1,3∗ Yen-Chun Chen2
1

Mengchen Liu2

Lu Yuan2

Leonid Sigal1,3,4

University of British Columbia 2 Microsoft
Vector Institute for AI 4 CIFAR AI Chair

arXiv:2407.14506v1 [cs.CV] 19 Jul 2024

3

{wancyuan, lsigal}@cs.ubc.ca
{yen-chun.chen, mengcliu, luyuan}@microsoft.com

Abstract
Recent studies customizing Multimodal Large Language Models (MLLMs) for
domain-specific tasks have yielded promising results, especially in the field of
scientific chart comprehension. These studies generally utilize visual instruction
tuning with specialized datasets to enhance question and answer (QA) accuracy
within the chart domain. However, they often neglect the fundamental discrepancy
between natural image-caption pre-training data and digital chart image-QA data,
particularly in the models’ capacity to extract underlying numeric values from
charts. This paper tackles this oversight by exploring the training processes necessary to improve MLLMs’ comprehension of charts. We present three key findings:
(1) Incorporating raw data values in alignment pre-training markedly improves
comprehension of chart data. (2) Replacing images with their textual representation
randomly during end-to-end fine-tuning transfer the language reasoning capability
to chart interpretation skills. (3) Requiring the model to first extract the underlying
chart data and then answer the question in the fine-tuning can further improve
the accuracy. Consequently, we introduce C HOPIN LLM, an MLLM tailored for
in-depth chart comprehension. C HOPIN LLM effectively interprets various types
of charts, including unannotated ones, while maintaining robust reasoning abilities.
Furthermore, we establish a new benchmark to evaluate MLLMs’ understanding
of different chart types across various comprehension levels. Experimental results show that C HOPIN LLM exhibits strong performance in understanding both
annotated and unannotated charts across a wide range of types.

1

Introduction

In today’s data-driven world, visualizations like bar and pie charts are crucial for deciphering complex
datasets. However, the increasing diversity and complexity of these charts highlights the need for
advanced tools to enhance human capabilities in data analysis. Artificial Intelligence (AI), particularly
Multimodal Large Language Models (MLLMs), is increasingly used to automate the understanding
of scientific charts, promising more efficient and accurate analysis. Robust benchmarks are also
essential, setting standards and metrics that drive the development and evaluation of these AI tools.
Prior studies have introduced end-to-end neural models aimed at enhancing chart comprehension [29, 31, 67], such as masked table prediction [67], chart question answering [38], and chart
de-rendering [31]. These models each is specialized in handling one task within the domain of chart
analysis. Furthermore, advancements in Multimodal Large Language Models (MLLMs), exemplified
by LLaVA [33, 34] and miniGPT [68], have showcased their versatility in vision-language tasks.
∗

The work is done during an internship at Microsoft.

Preprint. Under review.

000001_script_matplotlib_088.png

Annotated chart

Question

Annotated chart
GPT-4V
Gemini
ChartLlama
Ours

Extract the underlying
data for this chart
Predictions
by human

Predictions
by MLLMs

A‘21 A’22 B’21 B‘22 C’21 C’22
120 150 140 130 100 110

Non-annotated chart

Non-annotated chart
auxiliary line

ChartLlama

GPT-4V
Gemini
ChartLlama

A’21 A’22 B’21 B’22 C’21 C’22
105 125 120 100 85
90

Ours

A‘21 A’22 B’21 B‘22 C’21 C’22
120 150 140 130 100 110

Figure 1: The underlying data values can be inferred regardless of whether the chart is annotated.
However, existing MLLMs rely on annotations and struggle with unannotated charts. In contrast, our
model bridges this fundamental discrepancy between natural image-caption pre-training data and
digital chart image-QA data, enabling it to extract values regardless of whether the chart is annotated.
These generalist models undergo a two-stage training process: initially learning visual-language
alignment through image-caption pairs, followed by end-to-end fine-tuning using image-QA pairs.
This training not only enables LLMs to interpret visual data but also retains their extensive pre-trained
knowledge, which supports their reasoning abilities and leads to strong performance across diverse
visual language understanding tasks.
Recent advancements have further ignited interest in tailoring MLLMs to specialized domains such
as scientific chart understanding. Han et al. [22], Liu et al. [32] have explored collecting instructiontuned chart data and low-rank adaptation [24] to enhance MLLMs’ proficiency with unique chart
A
B
B
C to C
characteristics, research on the fundamental-training regimes –A namely,
pre-training
align across
2022 2021 2022
2021 2022 remains
modalities and comprehensive end-to-end fine-tuning – for2021
chart-specific
understanding
GPT4V
120 150 140 130 100 110
scarce. As shown in Fig. 1, existing MLLMs often struggle
to extract
the underlying data from charts
Gemini
120 140 105 130 85 110
when numerical values are not annotated. We hypothesize
that
this
issue
stems
ChartLlama 120 150 140
130from
100a gap
110 in visionlanguage alignment between natural image-caption pairs
and digital
chart-data
pairs.100
Without
Ours
120 150
140 130
110 targeted
pre-training for chart-data alignment, models may resort to relying on a “shortcut” of recognizing
numeric annotations through OCR during fine-tuning with QA pairs, rather than truly understanding
the visual subtleties of diverse charts.
This paper addresses the above issues by concentrating on the essential training methodologies for
MLLMs, including cross-modal feature alignment pre-training and comprehensive end-to-end finetuning. Our research is guided by the question, “How significantly does fundamental MLLM training
influence the enhancement of general MLLMs with chart-specific domain understanding?” Our
findings indicate that: (1) Raw data extraction are pivotal in alignment pre-training to bolster chart
data comprehension; (2) Substituting some chart images with purely textual data during end-to-end
fine-tuning not only preserves LLM’s text-only reasoning ability but also augments chart interpretation
capabilities; (3) Augmenting QAs with data extraction tasks in the fine-tuning phase allows model to
achieve the data prompting during testing, where it first extract data and then answer the QAs, further
improving the correctness of its reasoning skills. Furthermore, existing chart benchmarks are limited
in chart and question types. This prompt our introduction of a comprehensive chart benchmark, which
includes 18 chart types and three QA levels to better measure MLLM performance and advance
future research in this field.
Our key contributions are summarized as follows:
• We introduce C HOPIN LLM,2 a Multimodal Large Language Model tailored for comprehensive
chart understanding. This model excels at interpreting various chart types including unannotated
ones, underpinned by our detailed analysis and training guidance that emphasizes the importance
of foundational training for chart-specific tasks.
• We propose a novel data generation pipeline using text-only Large Language Models to efficiently
produce large-scale pairwise data. This approach significantly reduces the costs and complexity
of data generation for MLLM training.
• We establish a robust benchmark that includes a diverse array of chart types and question answering
levels, designed to rigorously evaluate MLLMs’ abilities in chart understanding.
2

Chart Oriented Pretraining Integration in Large Language Models

2

2
GPT4V
1
Gemini
ChartLlama t
Ours
1

2

Related works

Large language model Large Language Models (LLMs) have seen remarkable advancements in
recent years, primarily driven by transformers [54] that have significantly scaled in model size and
training data [6, 9, 11, 17, 19, 23, 49, 51]. These models excel in generalized reasoning and exhibit
robust chain-of-thought reasoning [55, 57, 65] across a variety of tasks, largely attributed to extensive
pre-training [3, 13, 66] and fine-tuning strategies [10, 43, 45]. The availability of using powerful
LLMs with specialized capabilities – ranging from general assistance [2, 20, 44, 53] to coding [21,
25, 50] – has fueled diverse applications such as data augmentation [14], data generation [46, 60],
and providing training training guidance [28, 61]. These developments have markedly accelerated
research and practical applications in the field.
Multimodal large language model Building on the success of LLMs, recent research has expanded
their application to multimodal tasks, including image [34, 35, 40, 64], video [7, 63], audio or
speech [4, 12, 18], mixed-modal [52], various tool and API usages [41, 47, 48], and robotics [5, 62].
In extending LLMs to image modalities, early studies combined LLMs with external vision models to
convert visual information into text, enhancing image comprehension [30, 59]. Others have integrated
visual encoders directly within LLM frameworks, developing end-to-end systems that transform
images into textual tokens [1, 8, 16, 33, 34, 68]. While maintaining capabilities like reasoning and
chain-of-thought processing across various tasks, these models often fall short in domain-specific
tasks like chart analysis [37, 42]. This prompts further research into specialized data collection and
fine-tuning for distinct domains.
Chart understanding Current approaches to chart understanding fall into two main categories:
models specifically designed for chart-related tasks [29, 31, 36, 38, 67], and those that utilize pretrained LLMs and MLLMs [22, 32, 39, 58]. The first group involves models trained exclusively on
chart-specific data, often limited by the scope of the training datasets thus cannot be applied to diverse
chart scenarios. The second group, which involves adapting existing LLMs and MLLMs through
fine-tuning [34] or integration with external models [30], shows promising versatility across various
questions and scenarios. Yet, there is a scarcity of research on MLLMs’ pre-training, crucial for deep
chart understanding and adaptability to multiple chart types in practical settings. Typically, chart
understanding models are evaluated against benchmarks focused on tasks like data extraction [26, 37],
summarization [27], and basic mathematical reasoning [42], which predominantly feature basic chart
types (e.g., bar, line, pie charts) and lack nuanced differentiation in QA levels to thoroughly assess
models’ understanding capabilities. Addressing these gaps, our work not only explores effective
pre-training strategies for MLLMs on chart data but also introduces a new benchmark with a variety
of chart types and differentiated QA levels (e.g., literal, inferential, reasoning) to evaluate MLLMs’
comprehensive abilities. Concurrently, CharXiv [56] is proposed for evaluating general understanding
of real-world scientific charts, including complex compositions with multiple subplots. In contrast,
our benchmark focuses on single-plot chart images, evaluating the raw data understanding and
mathematical reasoning of an MLLM.

3

Generating data for chart understanding

To build a chart understanding MLLM and study its fundamental training process, a comprehensive
dataset containing chart images paired with captions and raw data is essential for pre-training,
alongside different types of question-answer pairs for end-to-end fine-tuning. However, no existing
dataset provides the necessary variety of chart types, topics, and styles. To bridge this gap, we
introduce a novel data generation pipeline for large-scale chart data generation (Sec. 3.1) and QAs
generation (Sec. 3.2). With the data at hand, we then explore various training strategies in the later
sections, including feature alignment pre-training and end-to-end fine-tuning for LLMs. Figure 2
presents an overview of our framework.
3.1

Efficient data generation with quadratic scaling

Our data generation leverages the promising text content generation and coding abilities of large
language models, e.g., GPT-4, to generate chart images and data. Specifically, LLMs allow us to
synthesize raw data for chart images, and then the generated Python script turns the raw data into a
3

Line

chart type

Title: “ABC”
X_axis_label: “x-axis”
Y_axis_label: “y-axis”
Data: {
{“category”: “”, “value”: 0},
{“category”: “”, “value”: 0},
}

Stage 1: Feature alignment pre-training

Data expert

chart-description pairs, chart-json pairs

GPT-4

Chart topics

Json template
JSON expert

README

JSON
JSON

Sum. & Des.

QAs

Literal QAs

…

GPT-4

QAs

Generated data
Code expert

GPT-4

Stage 2: End-to-end ﬁne-tuning
General QAs: summary QAs + description QAs +
three-level QAs (literal, inferential, reasoning)
Augmented QAs: JSON-only QAs, + data-driven QAs

Inferential QAs

Stage 3: Downstream ﬁne-tuning

Reasoning QAs

high-quality downstream task instruction data

(LoRA)

(b)

LLM

…
Generated codes

ViT

(a)

projector

prompt

(c)

Figure 2: Overview of (a) the proposed data generation pipeline and (b) Training strategies of
C HOPIN LLM. Generating code and dataGeneral
points
QAsconforming to a shared JSON template enables
Chart
Outputs
quadratic scaling of the data sizeimage
(w.r.t. to #GPT calls).
The 3-stage training equips our model to
grasp the underlying data, thereby achieving a fundamental understanding of charts. (N and M
Raw respectively.)
denote the number of generated scripts and data,
data
Augmented QAs

chart image. In this way, we can produce image data without accessing costly multimodal LLMs like
GPT-4V. Unlike previous and concurrent works [22, 58] that prompt LLMs to iteratively generate
CSV data, QAs, and Python script for each chart image – a process that is costly to massively scale –
our pipeline features parallel code and data generation through shared templates and READMEs for
consistent definitions and formats across the same chart types. Most importantly, since all code script
and data share the same structure, our generated data can be universally applied to any generated
code and vice versa, significantly enhancing scalability without exhaustedly prompting LLMs. We
detail the pipeline further below.
Shared template and README As shown in Fig. 2 (a), given a chart type (e.g., line) sampled
from a predefined chart type database, the JSON expert GPT-4 first generates a JSON template for
the given chart type, along with a README file. In detail, the JSON template contains general
information for the chart image, including the title, x-axis, y-axis information, and raw data. The
README contains the definition of the chart type and the meanings of the keys and values to
enhance understanding of the JSON template. Please refer to Appendix F for some examples. We
note that the JSON template, together with the README, ensures the consistency of data generation
so that further data and code generation can follow the explicit format and definition guidance of the
template data. Note that we choose JSON as our primary data representation format, in contrast to
previous works [22, 37, 42, 58], which used CSV. The JSON format allows us to incorporate not
only numerical data but also additional chart information, such as titles and the scales of x and y
axes, which is beneficial for pair-wise pre-training tasks. Moreover, JSON data is structured, and
when paired with a README file, it minimizes ambiguity in data descriptions, which is particularly
valuable for complex chart types. For instance, in candlestick charts, we can clearly define a data
point as a dictionary containing “open”, “close”, “high”, and “low” values, rather than a list where
the meaning of each number might be unclear.
Orthogonal data and code generation With the template files at hand, we can generate data and
code independently. For the data generation branch, to ensure the generated data covers diverse
topics, we jointly input the produced template files (i.e., JSON template and README) and a topic
sampled from a pre-defined topic set (e.g., energy production and market share) into a data expert
GPT-4 module. For the complete topic list, please refer to Appendix G. We require the data expert
GPT-4 to follow the definitions in the template files and generate M JSON data along with different
kinds of questions and answers (e.g., summary QA) based on the raw data. As for code generation,
another code expert GPT-4 is utilized to produce N Python code based on the given chart type, data
template, and Python library. Note that to prevent generating simple code repeatedly for the given
chart type, we explicitly ask the code expert GPT-4 to introduce visual variations in aspects such as
color, legend, grid, font, and mark texture, etc. More details can be found in the appendix.
4

Chart Example 1

Literal QA

Tech Giant Quarterly Earnings Report

Q: What was the closing earnings per share (EPS) for Q2-2022?

Earnings per share (EPS) in USD

Inferential QA
Q: Was … closing EPS was lower than the opening EPS?

Short A: Yes

Long A: Yes, in Q4-2022 … of 6.40 USD was lower than the opening EPS of 6.50 USD.

Reasoning QA
Q: How much … the lowest of Q1-2022 to the highest of Q1-2023?

Short A: 1.80 USD

Long A: The EPS grew by 1.80 USD … Q1-2022 at 5.10 USD to …Q1-2023 at 6.90 USD.

Quarter

Chart Example 2

Literal QA

Website Trafﬁc vs Conversion Rate Analysis

Q: What is the website trafﬁc from Paid Advertisements?

Short A: 150

Long A: According to … Y-axis in the bar chart, the … is 150 (in thousands).
Conversion Rate (%)

Website Trafﬁc (in thousands)

Short A: 6.50 USD

Long A: The closing earnings per share (EPS) for Q2-2022 was 6.50 USD.

Monthly
Website
Visitors

Inferential QA
Q: Does Social Media Referrals lead in conversion rate?

Short A: Yes

Long A: Yes, Social … at 3.0%, the highest among all categories … line graph.
Monthly
Conversion
Rate

Reasoning QA
Q: What is the total website trafﬁc from all sources?

Short A: 660

Long A: The total … sum of … : 120,000 + 150,000 + 180,000 + 210,000 = 660,000.

Figure 3: Examples of generated three-level QAs with long and short answers, accessing the
understanding of charts from various perspectives. Best viewed in color.
3.2

Diverse QA synthesis

Based on the parallel data generation pipeline, we are able to collect massive amount of chart image
and JSON raw data pairs for the feature alignment pre-training. Now, we details how we generate
different types of QAs for end-to-end fine-tuning. Specifically, having each JSON data as input, we
use text-only LLM to generate question-answer (QA) pairs. To cover various question-anwser for
chart data, we include general QAs, containing not only description and summary QA but also three
different level of QAs: literal QAs, inferential QAs, and reasoning QAs (as illustrated in Fig. 3).
Furthermore, to enhance the training of chart understanding, we introduce two additional augmented
QAs (for training only): text-only QAs and data-driven QAs. We detail each QA type as follows:
•
•
•
•
•
•
•

Description QAs: Generate objective descriptions based on the chart data.
Summary QAs: Summarize the chart, highlighting key findings.
Literal QAs: Extract specific values directly from the data.
Inferential QAs: Infer global insights, such as identifying extreme values.
Reasoning QAs: Perform calculations to derive answers from chart data.
JSON-only QAs: Replace images with JSON raw data to augmented previous QAs.
Data-driven QAs: Prompt the model to extract JSON raw data before answering the question.

These QAs encompass a range of questions for chart images, covering abilities from basic data
understanding and global concept comprehension to advanced reasoning, allowing us to further
assess the abilities of MLLMs. Note that, for each QA pair, we use GPT-4 to generate both long
and short answers. The long answer, generated first, includes a step-by-step explanation to derive
the answer, while the short answer, generated later, contains only the final answer derived from the
long explanation. Short answers contain only numerical values or Yes/No response for convenient
evaluation purpose. For more examples of generated chart and QAs, please refer to Appendix J.
Composition for quadratically scaled data As shown in Fig. 3 (a), we consider 18 different chart
types. For each chart type, we collect 400 different Python codes (N = 400) and 1000 different
JSON data files (M = 1000) covering various topics. Note that we exclude bad data based on
predicted file structure’s correctness, Python code execution errors, and OCR tools. Please refer to the
supplementary materials for detailed information. After filtering, we have approximately 5 million
images, with the distribution for each chart type displayed in Fig. 3 (a). For each chart image, we
5

collect the raw data in JSON format, a shared README file, the corresponding Python script, 17
general question-answer (QA) pairs: one description QA, one summary QA, five literal QAs, five
inferential QAs, five reasoning QAs, 2 augmented QAs: 1 JSON-only QA, and 1 data-driven QA.
3.3

A new benchmark for comprehensive chart understanding

Existing chart benchmarks [37, 42] contains only a limited range of chart types (e.g., line, bar,
and pie charts) and lack of comprehensive QAs to access a model’s understanding of charts from
various perspectives, including raw data comprehension, inferential abilities, and mathematical
reasoning capabilities. To bridge this gap, we propose a comprehensive benchmark derived from the
aforementioned synthetic dataset. It covers 18 different chart types, three different levels of QAs
(literal, inferential, and reasoning QAs), and provides both long and short answers. Notably, the
chart images in the benchmark are not all annotated, allowing assessment of the model’s ability to
understand the underlying data of a chart as humans do. To ensure the quality of the images in the
benchmark, we employed human evaluations to filter the data and obtain a high-quality test set. The
evaluations are based on two criteria: Validity: Whether the essential components of the images (e.g.,
title, x, y-axis, labeling) are clearly visible and not missing. Extractability: Whether the evaluator can
extract the raw data from the given chart image, and whether the extracted data matches the generated
one. After human evaluation and filtering, we obtain a test set consisting of ≈ 300 image-QA paris
for each chart type, resulting in a total of ≈ 5k pairs. Note that these QAs equally cover literal,
inferential, and reasoning questions for measuring chart understanding of MLLMs.

4
4.1

Experiments and model analysis
Experimental setup

Benchmark Our evaluation utilizes four classical benchmarks to compare our model against
previous works. We specifically use the ChartQA dataset [37], which includes 1.5k chart images in its
test set, divided into human-written and machine-generated questions with 1.2k QA pairs each. The
human-written questions often require mathematical reasoning. ChartQA also provides CSV data for
each image, enabling us to conduct a Chart-to-Table (or Chart Extraction) task to assess the ability of
Multimodal Large Language Models (MLLMs) to extract raw data from charts, following previous
studies [22, 30]. Additionally, we use the PlotQA dataset [42] where images generally lack numerical
value annotations, necessitating value inference relative to the Y-axis. For evaluating the models’
capability to capture global concepts, we assess on the Chart-to-Text task using the Pew and Statista
splits from the dataset [27]. The Pew split contains 9k images accompanied by descriptions written
by professional editors, while the Statista split includes 33k images that often feature descriptive text
within the charts themselves, making it an easier split than Pew.
Metrics For ChartQA and PlotQA, we adopt the relaxed accuracy metric for numeric answers,
allowing a 5% margin of error from the exact value, and use exact match for non-numeric answers as
per the standard in previous studies [22, 37]. In the Chart-to-Table task, we measure performance
using F1 score of Relative Mapping Similarity (RMS) and Relative Number Set Similarity (RNSS)
to evaluate numeric accuracy and table similarity, respectively. For the Chart-to-Text task, we use
BLEU-4, an N-gram matching score, following the previous work [27].
A 3-stage training process Unlike previous approaches that convert a general MLLM into a
chart-specific expert by only applying LoRA fine-tuning on limited high-quality data [22], training
C HOPIN LLM unfolds in three stages, illustrated in Fig. 2 (b). The 3-stage training enables our model
not only to understand chart QAs and downstream tasks but also to capture the underlying data,
thereby achieving a fundamental understanding of charts. In the initial pre-training stage, we fix the
ViT and LLM while training the projector from scratch using original LLaVA data alongside our newly
generated chart-description and chart-json pairs. The second stage involves freezing ViT and jointly
fine-tuning the projector and LLM with both original LLaVA QA pairs and our generated chart QA
pairs, enabling the LLM to comprehend visual tokens and facilitate chart question answering. Finally,
we apply LoRA fine-tuning to align the LLM’s response distribution with the target downstream
dataset. Each stage is carefully studied and the results are presented in the following subsections. In
the following study, we ablate 1 stage at a time and use the full-training setting for the other 2 stages.
6

Table 1: Ablation of stage-1 training. This empirically verifies that pre-training basic chart visual
perception is still important, even with abundant stage-2 instruction fine-tuning data. Moreover,
learning to predict JSON data is beneficial, even on top of pre-training with descriptive captions.
Training data
LLaVA-CC3M-Pretrain pairs [34]
+ Chart-description pairs
+ Chart-JSON data pairs

Our benchmark

ChartQA
human

augmented

literal

inferential

reasoning

44.80
48.56
52.28

83.92
86.89
87.68

41.45
42.71
44.96

34.09
33.68
34.94

22.31
23.51
24.61

Table 2: Ablation of stage-2 training. Each type of new instruction / QA data improves the final
performance consistently across almost all metrics. Best result is highlighted in Bold and the second
best is underlined. † denotes inference technique without extra data.
Training data
LLaVA-Instruct-150K QAs
+ description and summary QAs
+ Literal / infer. / reasoning QAs
+ JSON-only QAs
+ Data-driven QAs
+ Data Prompting†

4.2

Our benchmark

ChartQA
human

augmented

literal

inferential

reasoning

45.84
47.04
48.96
49.60
52.28

86.48
87.76
87.52
87.36
87.68

16.54
19.90
40.55
41.45
44.96

15.99
15.69
33.33
34.84
34.94

6.57
5.26
21.30
22.36
24.61

56.96

87.60

52.00

41.75

31.90

Stage 1: Pre-training for chart feature alignment

In the first training stage, the goal is to align visual and linguistic features so that visual data can be
seamlessly translated into the textual domain for LLM comprehension. Employing a strategy from
Liu et al. [34], we use a projector to translate visual features from ViT [15] into the textual domain,
training it with pairwise image-caption data to enhance its capability to capture visual information.
We explore three configurations: utilizing only LLaVA CC3M Pretraining data,3 combining LLaVA
data with chart-description pairs, and using LLaVA data with both chart-description and chart-raw
data pairs. The data for stage two training remains consistent across these settings, summary QAs,
description QAs, three-level QAs, text-only QAs, and data-driven QAs, as depicted in Fig. 2 (b). In
stage three, all models undergo LoRA fine-tuning on the downstream dataset, using LLaVA-7B as
the baseline for this comparison. Results are detailed in Table 1.
Dense data alignment is beneficial for both chart data comprehension and reasoning. For chart
images, chart-description pairs act as standard image-caption pairs. However, to more effectively
bridge the visual-textual gap, we also utilize chart-json pairs that encompass the underlying numerical data and its schema of the charts. This approach not only aligns visual features with textual
descriptions but also significantly enhances model performance, as demonstrated by improvements of
approximately 2% in literal QAs and about 1% in reasoning skills, according to results in Table 1.
4.3

Stage 2: End-to-end fine-tuning

The second stage, end-to-end fine-tuning, trains the MLLM to actually understand the aligned visual
tokens so that it follows the user instruction and reason about the answer, on top of the inherent
language capability from the original LLM. We utilize a significant number of image-QA pairs to
jointly tune the LLM and the projector. To evaluate the effectiveness of incorporating chart QAs
during fine-tuning, we conduct ablation studies starting with a baseline that uses only LLaVA Instruct150K data,4 incrementally adding extra QA pairs. All methods leverage the same pre-training weights,
derived from training on LLaVA data with both chart-description and chart-raw data pairs (the best
3
4

https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K
https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K

7

Table 3: Comprehensive evaluation across four chart benchmarks. C HOPIN LLM achieves best
QA results on both (mostly) annotated benchmark, ChartQA, and non-annotated benchmark, PlotQA.
H and A denote the human and augmented branch in ChartQA, respectively. Stat. represent the
statista split. † : our reproduction using the official code. Note that for fair comparison, we don’t use
chain-of-reasoning in the inference. The best result is highlighted in Bold and the second underlined.
ChartQA

Method

Chart-to-Table

Chart-to-Text

PlotQA∗

H

A

Avg.

F1

RNSS

Pew

Stat.

v1

v2

Pix2struct [29]
Matcha [31]
Unichart [38]
DePlot [30]

30.50
38.20
43.92
-

81.60
90.20
88.56
-

56.00
64.20
66.24
-

52.71
87.22

94.28

10.30
12.20
12.48
-

38.00
39.40
38.21
-

-

-

LLaVA7B † [34]
LLaVA13B
LLaVA13B †
ChartLlama13B [22]

36.00
37.68
42.56
48.96

67.44
72.96
73.60
90.36

51.72
55.32
58.08
69.66

56.96
48.95
63.18
89.84

91.83
93.18
94.65

8.50
7.16
8.83
14.23

21.50
24.65
22.39
40.71

27.26
27.68
29.76

30.64
30.98
29.93

C HOPIN LLM 7B
C HOPIN LLM 13B

52.28
54.11

87.68
88.67

69.98
71.39

83.63
88.12

95.27
95.95

11.50
12.66

38.97
40.81

30.06
33.98

31.08
33.96

setting in Sec. 4.2. In stage three, all models undergo LoRA fine-tuning on the downstream dataset.
Table 2 presents the result.
JSON-only QAs allow transferring pure text reasoning abilities to multimodal chart understanding. The chart understanding of MLLMs can be seen as two stages: visual and text raw data
alignment (which is done in the training of the first stage) and question answering with reasoning
ability on the raw textual data (JSON). Thus, with a well-aligned first stage training, we hypothesize
that re-blending some pure textual QAs, preserving the ability of reasoning on text raw data, can
also benefit the reasoning abilities in visual-text scenarios. As detailed in Sec. 3.2, for JSON-only
QAs, rather than utilizing chart images and QAs, we replace the chart image with JSON data and a
README, resulting in purely text-based QAs for training. Table 2 demonstrates the effectiveness of
each QA type. We discover that re-blending JSON-only data during the end-to-end fine-tuning stage
improves chart reasoning skills, matching the assumption.
Data-driven QAs in the fine-tuning stage enable MLLMs to enhance prediction accuracy
through data prompting. As detailed in Sec. 3.2, data-driven QAs are multi-turn QAs, which
require models to extract raw data before answering given questions. Combined with the raw data
reasoning abilities enhanced via JSON-only QAs, the model can perform data prompting during
inference, where models achieve better reasoning robustness by first extracting raw data and then
answering the given question based on the data. Please refer to Appendix I for some examples.
As shown in Table 2, data-driven QAs significantly enhance the model’s ability to capture visual
information. Furthermore, leveraging data prompting in inference significantly improves performance
across all downstream tasks.
4.4

Stage 3: Downstream fine-tuning

We build C HOPIN LLM with the best setting based on the previous observation (the data used in each
stage can be refered to Fig. 2 (b)), and we compare C HOPIN LLM with existing chart understanding
approaches, including Pix2struct [29], Matcha [31], Unichart [38], Deplot [30], LLaVA [34], and
ChartLlama [22]. The results are shown in Table 3.
Classical question-answering on ChartQA. We find that C HOPIN LLM achieves the best performance on the human split of ChartQA, as shown in Table 3, with ≈ 5% improvement, while achieving
comparable accuracy on the augmented split with the previous state-of-the-art model, ChartLlama,
making C HOPIN LLM the best model in terms of average accuracy. We note that the human split
in ChartQA is more challenging than the augmented split, as it contains more reasoning questions,
suggesting that C HOPIN LLM is better at performing reasoning tasks.
8

Chart-to-Table

Gold table

Ours

ChartLlama

LLaVA

Entity

Value

Entity

Value

Entity

Value

Entity

Value

Portugal

84.0

Portugal

84.9

Portugal

84.0

Portugal

84.9

Italy

80.3

Italy

80.3

Italy

80.3

Italy

80.3

France

83.0

France

83.7

France

83.0

France

88.7

UK

915.0

UK

91.5

UK

915.0

UK

91.5

U.S

92.9

U.S

92.9

U.S

92.9

U.S

92.9

GT: In France, home to Europe’s largest Jewish population, reports of anti-Semitic incidents rose dramatically in 2018. There

Chart-to-Text

were 541 cases reported last year – not as high as in some previous years, but a 74% increase from 2017, according to the
country’s Ministère de l’Intérieur. And already in 2019, there have been several new high-proﬁle anti-Semitic incidents, including
swastikas being spray-painted on graves in a Jewish cemetery.
Ours: France has seen more anti-Semitic attacks than any other country in Europe, according to a 2018 report by the French
government. The report, which tracked reported “acts” – including hate crimes, hate speech and anti-Semitic acts by individuals
against people and property – found that 541 such acts occurred in 2018, up from 311 in 2017.
ChartLlama: The chart illustrates the number of reported anti-Semitic acts in France from 2011 to 2018. The data shows a
signiﬁcant increase in the number of reported acts from 2011 to 2018. In 2011, there were 432 reported acts, which increased to
783 in 2018. The number of reported acts peaked in 2015 with 851, and then decreased to 651 in 2016. The chart also indicates a
slight increase in 2017 with 653 reported acts.

Figure 4: Qualitative comparison on Chart-to-text and Chart-to-Table task. C HOPIN LLM
effectively captures the underlying data and generates accurate chart descriptions with correct
numerical values. Green denotes correct numbers, while red indicates mistakes. Best viewed in color.
Note that the gold table is the noisy ground truth provided in ChartQA dataset.
ChartLlama
Can you please extract the raw data into csv format?
"Household Income","Income Distribution"\n "1000-2000",1500\n "2000-2500",4100\n
"2500-3500",5000\n "3500-4500",4100\n "4500-5500",2200\n "5500-7500",2000\n
"7500-10000",1200\n "10000-15000",900\n "15000+",2400
By how much does the frequency … 80k-100k income range?
5000, The calculation is as follows: 60k-80k households = 2200\n 80k-100k households =
1200, and 2200 - 1200 = 1000.\n 1000 is 5000 fewer households than the 60k-80k range, so
the decrease is 5000.

Ours

GT
By how much does the frequency of
households decrease when moving
from the 60k-80k income range to the
80k-100k income range?
The frequency of households
decreases by 1100 when moving from
the 60k-80k income range (4300
households) to the 80k-100k income
range (3200 households).

Can you please extract the raw data into csv format?
Annual Household Income (Thousands USD),Number of households\n <20k%,1500\n
20k-40k%,4700\n 40k-60k%,5000\n 60k-80k%,4300 \n 80k-100k%,3200 \n
100k-120k%,2100 \n 120k-140k%,1100\n >140k,900
By how much does the frequency … 80k-100k income range?
The frequency of households decreases by 4300 - 3200 = 1100 households when moving from
the 60k-80k income range to the 80k-100k income range.

Figure 5: Qualitative comparison of multi-turn chart question-answering. Green denotes numbers
that match groundtruth number, while red indicates mismatches. Best viewed in color.

Raw data and global concept understanding. As listed in Table 3, C HOPIN LLM achieves the
second-best F1 score and the highest RNSS result, indicating that C HOPIN LLM can capture not only
the structure but numerical values of raw data of chart images. We note that the performance on the
chart-to-table task may have been saturated, as the images are mostly annotated. In this context, this
primarily measures the OCR capability and does not assess the ability to capture the underlying data.
As for the Chart-to-Text, shown in Table 3, C HOPIN LLM performs comparable in the global concept
capturing and can caption chart image with meaningful texts.
Performance on unannotated chart images. Most of the images in ChartQA [37] are annotated,
which means the numerical values of data points are explicitly shown on the images. We observe that
existing chart MLLMs, such as ChartLlama [22], seem to heavily rely on this annotation for chart
understanding, which is not ideal since real-world charts may be unannotated. We further evaluate
them using the PlotQA dataset, and the results are shown in the last column of Table 3. Notably,
since training previous models like ChartLlama on PlotQA is infeasible, we load the model weights
as used in ChartQA and perform zero-shot prediction on PlotQA. The results show that our model
performs significantly better (≈ 3% improvement) on unannotated chart images, suggesting that our
methods with fundamental training rely less on numerical annotations on chart images.
9

Table 4: Performance comparison on different chart types. Overlapped denotes the chart types that
are in both the ChartLlama training set and our dataset.
Basic

Method

Overlapped

Line

Bar

Pie

Funnel

Gantt

Heatmap

Scatter

Box

Candle.

LLaVA [34]
21.0
ChartLlama [22] 27.6
C HOPIN LLM
48.0

18.0
16.7
37.0

27.0
32.7
62.7

17.6
26.0
52.6

9.3
12.0
24.0

16.3
16.7
30.0

7.7
8.0
24.6

13.3
12.0
25.7

23.0
26.3
41.3

4.5

More model analysis

Qualitative examples We provide a qualitative comparison of chart-to-text and chart-to-table tasks,
with results depicted in Fig. 4. In the chart-to-table task, our model accurately captures values from
chart images, unlike LLaVA and ChartLlama. It is important to note that the gold data tables for
ChartQA are not always directly accessible, leading to the use of existing models or OCR tools for
data extraction. This process can introduce errors, such as misreporting the value 91.5 for the UK as
915.0, which can adversely affect the performance of MLLMs fine-tuned on such data. Despite these
dataset inaccuracies, our model remains robust, correctly outputting values where ChartLlama does
not. In the chart-to-text comparison, both ChartLlama and our model grasp the overall concept of the
charts, but our model excels at accurately capturing and summarizing exact numerical values.
Additionally, as a multimodal chatbot, we emphasize preserving human-like multi-turn conversation
abilities. Figure 5 presents a qualitative comparison on chart images with multi-turn QAs. Although
ChartLlama extracts accurate numerical values, it fails to provide coherent explanations or reasonable
text outputs. In contrast, C HOPIN LLM not only extracts accurate data but also provides logical
reasoning and coherent explanations, showcasing the effectiveness of our training approach.
Performance across different chart types Our model, trained extensively across a variety of
chart types, was evaluated to assess its performance against the previous state-of-the-art model on
the same chart types. For an unbiased comparison, we focused on the short answer format in QA
pairs to avoid variations in output preference. The results, detailed in Table 4, reveal that our model
consistently outperforms the state-of-the-art across both overlapping and basic chart types. Notably,
our benchmark, which features unannotated images, poses a greater challenge than ChartQA. The
substantial performance improvement indicates that our model is adept at inferring data directly from
charts and demonstrates superior reasoning capabilities.

5

Conclusion

In this paper, we explore the impact of fundamental training strategies in adapting generalist Multimodal Large Language Models (MLLMs) to chart understanding. We offer practical guidance for
optimizing feature alignment pre-training and end-to-end fine-tuning. Leveraging these enhanced
training strategies, we introduce a specialized chart MLLM, named C HOPIN LLM, capable of interpreting diverse chart types independently of numerical annotations. Extensive experiments confirm
that C HOPIN LLM surpasses the previous state-of-the-art across four benchmarks, validating our
framework’s effectiveness. Additionally, we present a new benchmark specifically designed to
evaluate MLLMs’ comprehension across various chart types and multiple levels of understanding.

Acknowledgement
This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs, NSERC
CRC, and NSERC DGs. Resources used in preparing this research were provided, in part, by the
Province of Ontario, the Government of Canada through CIFAR, the Digital Research Alliance of
Canada,5 companies sponsoring the Vector Institute,6 and Advanced Research Computing at the
University of British Columbia. Additional hardware support was provided by John R. Evans Leaders
Fund CFI grant and Compute Canada under the Resource Allocation Competition award.
5
6

alliance.can.ca
https://vectorinstitute.ai/#partners

10

Bibliography
[1] E. Aiello, L. Yu, Y. Nie, A. Aghajanyan, and B. Oguz. Jointly training large autoregressive multimodal
models. arXiv preprint arXiv:2309.15564, 2023. 3
[2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. Technical Report, 2023. 3
[3] I. Beltagy, K. Lo, and A. Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint
arXiv:1903.10676, 2019. 3
[4] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul,
D. Grangier, M. Tagliasacchi, et al. Audiolm: a language modeling approach to audio generation.
IEEE/ACM transactions on audio, speech, and language processing, 2023. 3
[5] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey,
C. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv
preprint arXiv:2307.15818, 2023. 3
[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 3
[7] G. Chen, Y.-D. Zheng, J. Wang, J. Xu, Y. Huang, J. Pan, Y. Wang, Y. Wang, Y. Qiao, T. Lu, et al. Videollm:
Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292, 2023. 3
[8] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman,
X. Wang, Y. Tay, et al. Pali-x: On scaling up a multilingual vision and language model. arXiv preprint
arXiv:2305.18565, 2023. 3
[9] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,
S. Gehrmann, et al. Palm: Scaling language modeling with pathways. JMLR, 2023. 3
[10] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma,
et al. Scaling instruction-finetuned language models. JMLR, 2024. 3
[11] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language
models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. 3
[12] N. Das, S. Dingliwal, S. Ronanki, R. Paturi, D. Huang, P. Mathur, J. Yuan, D. Bekal, X. Niu, S. M. Jayanthi,
et al. Speechverse: A large-scale generalizable audio language model. arXiv preprint arXiv:2405.08295,
2024. 3
[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805, 2018. 3
[14] B. Ding, C. Qin, R. Zhao, T. Luo, X. Li, G. Chen, W. Xia, J. Hu, A. T. Luu, and S. Joty. Data augmentation
using llms: Data perspectives, learning paradigms and challenges. arXiv preprint arXiv:2403.02990, 2024.
3
[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929, 2020. 7
[16] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,
T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 3
[17] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, et al.
Glam: Efficient scaling of language models with mixture-of-experts. In ICML, 2022. 3
[18] Y. Fathullah, C. Wu, E. Lakomkin, J. Jia, Y. Shangguan, K. Li, J. Guo, W. Xiong, J. Mahadeokar, O. Kalinli,
et al. Prompting large language models with speech recognition abilities. In ICASSP, 2024. 3
[19] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple
and efficient sparsity. JMLR, 2022. 3
[20] G. Gemini Team. Gemini: A family of highly capable multimodal models. technical report. Technical
Report, 2023. 3
[21] O. GitHub. Github copilot. URL https://github.com/features/copilot. 3

11

[22] Y. Han, C. Zhang, X. Chen, X. Yang, Z. Wang, G. Yu, B. Fu, and H. Zhang. Chartllama: A multimodal llm
for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023. 2, 3, 4, 6, 8, 9, 10, 17
[23] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint
arXiv:2203.15556, 2022. 3
[24] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank
adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2
[25] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel,
G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 3
[26] S. Kantharaj, X. L. Do, R. T. K. Leong, J. Q. Tan, E. Hoque, and S. Joty. Opencqa: Open-ended question
answering with charts. arXiv preprint arXiv:2210.06628, 2022. 3
[27] S. Kantharaj, R. T. K. Leong, X. Lin, A. Masry, M. Thakkar, E. Hoque, and S. Joty. Chart-to-text: A
large-scale benchmark for chart summarization. arXiv preprint arXiv:2203.06486, 2022. 3, 6
[28] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh. Reward design with language models. arXiv preprint
arXiv:2303.00001, 2023. 3
[29] K. Lee, M. Joshi, I. R. Turc, H. Hu, F. Liu, J. M. Eisenschlos, U. Khandelwal, P. Shaw, M.-W. Chang, and
K. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In ICML,
2023. 1, 3, 8
[30] F. Liu, J. M. Eisenschlos, F. Piccinno, S. Krichene, C. Pang, K. Lee, M. Joshi, W. Chen, N. Collier,
and Y. Altun. Deplot: One-shot visual language reasoning by plot-to-table translation. arXiv preprint
arXiv:2212.10505, 2022. 3, 6, 8
[31] F. Liu, F. Piccinno, S. Krichene, C. Pang, K. Lee, M. Joshi, Y. Altun, N. Collier, and J. M. Eisenschlos.
Matcha: Enhancing visual language pretraining with math reasoning and chart derendering. arXiv preprint
arXiv:2212.09662, 2022. 1, 3, 8
[32] F. Liu, X. Wang, W. Yao, J. Chen, K. Song, S. Cho, Y. Yacoob, and D. Yu. Mmc: Advancing multimodal
chart understanding with large-scale instruction tuning. arXiv preprint arXiv:2311.10774, 2023. 2, 3
[33] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. arXiv preprint
arXiv:2310.03744, 2023. 1, 3
[34] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. NeurIPS, 2024. 1, 3, 7, 8, 10
[35] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, et al. Deepseek-vl:
towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 3
[36] A. Masry and E. Hoque. Integrating image data extraction and table parsing methods for chart question
answering. In Chart Question Answering Workshop, in CVPR, 2021. 3
[37] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. Chartqa: A benchmark for question answering
about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 3, 4, 6, 9
[38] A. Masry, P. Kavehzadeh, X. L. Do, E. Hoque, and S. Joty. Unichart: A universal vision-language
pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. 1, 3, 8
[39] A. Masry, M. Shahmohammadi, M. R. Parvez, E. Hoque, and S. Joty. Chartinstruct: Instruction tuning for
chart comprehension and reasoning. arXiv preprint arXiv:2403.09028, 2024. 3
[40] B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng,
F. Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint
arXiv:2403.09611, 2024. 3
[41] MeetkAI. Functionary. URL https://functionary.meetkai.com/. 3
[42] N. Methani, P. Ganguly, M. M. Khapra, and P. Kumar. Plotqa: Reasoning over scientific plots. In WACV,
2020. 3, 4, 6
[43] OpenAI. Chatgpt: Optimizing language models for dialogue. Technical report., 2023. 3
[44] OpenAI. Gpt-4 technical report. Technical Report, 2023. 3

12

[45] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 35:
27730–27744, 2022. 3
[46] A. Patel, C. Raffel, and C. Callison-Burch. Datadreamer: A tool for synthetic data generation and
reproducible llm workflows. arXiv preprint arXiv:2402.10379, 2024. 3
[47] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive
apis. arXiv preprint arXiv:2305.15334, 2023. 3
[48] S. G. Patil, T. Zhang, V. Fang, R. Huang, A. Hao, M. Casado, J. E. Gonzalez, R. A. Popa, I. Stoica,
et al. Goex: Perspectives and designs towards a runtime for autonomous llm applications. arXiv preprint
arXiv:2404.06921, 2024. 3
[49] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv
preprint arXiv:2112.11446, 2021. 3
[50] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al.
Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. 3
[51] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,
G. Zerveas, V. Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a
large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. 3
[52] C. Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818,
2024. 3
[53] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,
S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
2023. 3
[54] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. NeurIPS, 2017. 3
[55] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency
improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 3
[56] Z. Wang, M. Xia, L. He, H. Chen, Y. Liu, R. Zhu, K. Liang, X. Wu, H. Liu, S. Malladi, et al. Charxiv:
Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521, 2024.
3
[57] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought
prompting elicits reasoning in large language models. NeurIPS, 35:24824–24837, 2022. 3
[58] R. Xia, B. Zhang, H. Ye, X. Yan, Q. Liu, H. Zhou, Z. Chen, M. Dou, B. Shi, J. Yan, et al. Chartx &
chartvlm: A versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint
arXiv:2402.12185, 2024. 3, 4
[59] Z. Yang, Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, and L. Wang. An empirical study of gpt-3 for few-shot
knowledge-based vqa. In AAAI, 2022. 3
[60] Y. Yu, Y. Zhuang, J. Zhang, Y. Meng, A. J. Ratner, R. Krishna, J. Shen, and C. Zhang. Large language
model as attributed training data generator: A tale of diversity and bias. NeurIPS, 2024. 3
[61] W. Yuan, R. Y. Pang, K. Cho, S. Sukhbaatar, J. Xu, and J. Weston. Self-rewarding language models. arXiv
preprint arXiv:2401.10020, 2024. 3
[62] F. Zeng, W. Gan, Y. Wang, N. Liu, and P. S. Yu. Large language models for robotics: A survey. arXiv
preprint arXiv:2311.07226, 2023. 3
[63] H. Zhang, X. Li, and L. Bing. Video-llama: An instruction-tuned audio-visual language model for video
understanding. arXiv preprint arXiv:2306.02858, 2023. 3
[64] P. Zhang, X. D. B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, H. Yan,
et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and
composition. arXiv preprint arXiv:2309.15112, 2023. 3
[65] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola. Multimodal chain-of-thought reasoning in
language models. arXiv preprint arXiv:2302.00923, 2023. 3

13

[66] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. A
survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 3
[67] M. Zhou, Y. R. Fung, L. Chen, C. Thomas, H. Ji, and S.-F. Chang. Enhanced chart understanding in vision
and language task via cross-modal pre-training on plot table pairs. arXiv preprint arXiv:2305.18641, 2023.
1, 3
[68] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding
with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 3

14

Appendix
A Details of the dateset generation
A.1 Dataset filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Details of expert GPT-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16
16
16

B Implementation details

16

C Experimental results

17

D Social impact

18

E Limitation

18

F Examples for JSON template and READMEs
F.1 Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F.2 Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19
19
19

G Examples for pre-defined topics

20

H Examples of augmented QAs
H.1 JSON-only QA: example 1
H.2 JSON-only QA: example 2
H.3 Data-driven QA: example 1
H.4 Data-driven QA: example 2

.
.
.
.

21
21
21
22
22

I

Examples of data prompting
I.1 Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
I.2 Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23
23
23

J

Examples from our benchmark
J.1 Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
J.2 Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
J.3 Example 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24
24
25
26

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

15

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

A
A.1

Details of the dateset generation
Dataset filtering

In Section 3.1, we introduce a novel data generation pipeline that leverages text-only LLMs. This
pipeline enables us to collect chart images along with various data and QA pairs without extensive
human effort, thereby reducing the cost of creating pairwise data. However, LLMs are not perfect
and can make mistakes in either data generation or code script generation. Thus, in this section,
we discuss the data filtering techniques we use to improve the quality of the synthetic dataset. The
generation pipeline is split into three parts: shared template generation, data and QA generation, and
code script generation. We now detail the filtering process for each part.
Shared template and README The shared template and README file for each chart type form
the core of the entire data generation process, as the subsequent raw data, QA, and Python script are
based on the shared template. Therefore, for the shared template, we deploy a human check to ensure
the template contains necessary elements for the chart (i.e., title, x-axis, y-axis, data). Additionally,
humans are required to verify the correctness of the chart type definitions in the README. Note that
we consider 18 different chart types; thus, there are 18 template JSON files along with the READMEs
in our dataset.
Data and QA generation Since all the data should follow the template JSON, for the data generation part, we apply filtering based on the JSON structure. Specifically, we remove generated data that
deviates from the template file by comparing the elements in the keys of the JSON dictionary and the
data types of all the values. As for QA generation, we check the structure of the output dictionary. In
detail, the keys of the output QA dictionary should contain summary, description, literal, inferential,
and reasoning QAs. We filter out QAs with missing attributes.
Code script generation We predefined a code expert GPT-4 to use four different Python libraries
to plot the chart images: Matplotlib, Plotly, Pygal, and Seaborn. The advantage of these libraries is
that if the Python code or input data is incorrect in terms of structure or other errors, the generated
image will either be missing with a Python error or display a "No Data" icon. Thus, we apply a
two-step filtering process: (1) Python Error Filtering: If there is an error while running the Python
script to generate the image, we will remove the script and the corresponding JSON data. (2) OCR
Tool Filtering: If the image is generated but there is some other error, the output image will display a
"No Data" icon on it. To this end, we further use an OCR tool to detect whether there is any "No
Data" icon in the images. If so, we will remove the data and script accordingly.
A.2

Details of expert GPT-4

In the GPT-4 module, before feeding the prompts for generation, we must specify the system message
for GPT-4. These system messages explicitly inform GPT-4 about the environment for the generation
and the role it must play in this task, helping the model to output precise responses that match users’
needs. Thus, in our data generation pipeline, we encounter three different circumstances, and we
have three different system messages for GPT-4, tailoring it to become an expert accordingly. We
provide the details of these system message in Table 5 for reference.

B

Implementation details

In this paper, if not specified, we employ the same framework as LLaVA-7B for our model architecture.
For details regarding the model architecture, hyperparameters, and the type of optimizer, we adhere to
the configuration settings found in the official LLaVA repository.7 Regarding computational resources
for training, we use 8*A100 GPUs for both the pretraining and finetuning stages. Considering the
amount of data utilized in these stages, it takes approximately one day for pretraining and four days
for finetuning using 8*A100 GPUs.
16

Table 5: System messages for GPT-4.
Model

System messages

JSON expert GPT-4

You are an AI chatbot designed to help users generate a JSON template file
along with a README for a specific chart type. Once users specify the
chart type, you will have to determine the necessary attributes for plotting a
chart, including but not limited to the title, x-axis, y-axis, and data. The data
part should have a general structure that covers both simple and complex
examples for the chart type. As for the README, it should explain the
meaning and type of each attribute or label, and also the definition for this
chart type. The generated template and README will then be used in
creating raw data and a Python script for visualizing the chart.

Data expert GPT-4

You are an expert in generating question and answer pairs based on raw
chart data. Your role involves carefully examining chart data, which is
presented in JSON format, and creating relevant question and answer pairs.
These pairs will be used for instructional tuning of a vision and language
model. Along with the chart data, you will receive a JSON template and
a README file that provides additional information on the meaning of
each attribute in the JSON data. It is important to review all the provided
materials thoroughly to ensure the question and answer pairs are accurate
and useful for model training.

Code expert GPT-4

You are a Python code AI assistant and good at chart image plotting. Now,
you are asked to modify the Python code to a general version that can take
any JSON data matching the definition in the template.

Table 6: Performance comparison with synthetic data using in different training stages.
Idx
(1)
(2)
(3)

C

Model
LLaVA-7B
LLaVA-7B
LLaVA-7B

Use syn. data in
stage 3
stage 1 & 2

Our benchmark

ChartQA
human

augmented

literal

inferential

reasoning

36.00
37.60
52.28

67.44
70.40
87.68

14.69
27.57
45.91

18.50
29.17
34.74

7.32
12.48
23.01

Experimental results

Effectiveness of synthetic data in different training stages. In the previous section, we showcase
that chart data generated by text-only LLMs enhances MLLM learning in chart understanding. Here,
we compare the performance of using the same data amount in the third stage of LoRA downstream
fine-tuning, similar to Chartllama [22]. The results are in Table 6. Specifically, Model (1) is the
baseline, trained with LLaVA data in pre-training and fine-tuning stages, followed by LoRA finetuning on ChartQA. Model (2) is similar but includes synthetic data with ChartQA in the LoRA
fine-tuning stage. Model (3), our best 7B model, uses the same synthetic data as Model (2) but
incorporates it in pretraining and fine-tuning before LoRA fine-tuning on ChartQA. As shown in
Table 6, Model (2) shows an ≈ 2% improvement over Model (1), indicating the benefit of synthetic
data. However, Model (3) shows a significant improvement over Model (2), suggesting synthetic data
is more effective in fundamental training rather than fine-tuning. We propose two possible reasons
for this. First, alignment issues cannot be effectively resolved via LoRA tuning, as it only adjusts a
small portion of the model’s parameters. Second, the output preference of synthetic data may differ
from that of the downstream dataset. Joint tuning might shift the output preference away from the
downstream task, resulting in limited performance improvement.

7

LLaVA: https://github.com/haotian-liu/LLaVA.

17

D

Social impact

Our model is capable of chart understanding and can interpret the raw data of a chart like a human,
without relying on annotations, while also performing various levels of QA tasks. Thus, our model
can be used in many data analysis scenarios, such as market research, healthcare trend analysis,
and other data science areas. With the help of our model, humans can process large volumes of
chart data more efficiently, make informed decisions, and enhance reporting accuracy. While our
model provides benefits in chart understanding and analysis, there are potential negative impacts. For
instance, it could be employed to create misleading data visualizations or generate false narratives
when combined with other LLM tools. These fake charts and pieces of information can negatively
affect decision-making processes.

E

Limitation

In this paper, we propose an MLLM model for chart understanding, fundamentally trained on
synthetic data. However, since the synthetic data generated by LLMs cannot be perfect, sometimes
incorrect data can be introduced into the dataset and may not be filtered out by our filtering process.
These data can result in misalignments and incorrect mappings during pre-training and fine-tuning,
potentially leading to incorrect responses and hallucinations. Thus, the performance of our chart
MLLMs is limited by the LLMs’ generation capabilities. We can potentially include more advanced
LLMs in the data generation pipeline to reduce the occurrence of incorrect data. Moreover, another
limitation of our model is that it currently supports understanding only 18 chart types. However,
there are many more chart types in the real world. Developing an open-domain, versatile chart
understanding MLLM remains a task for future work.

18

F

Examples for JSON template and READMEs

F.1

Example 1
Template
JSON

{ "chart_title": "Chart Title", "x_axis_label": "X-Axis Label", "y_axis_label": "Y-Axis Label", "data": [
{ "category": "Category 1", "value": 0 }, { "category": "Category 2", "value": 0 }, { "category":
"Category 3", "value": 0 } ] }
The JSON template provided below is designed to generate data for a 'Bar Chart' chart. This template
includes all the necessary attributes and labels to ensure consistent data formatting for the chart.
- `chart_title`: This attribute represents the title or name of the bar chart. Please replace the placeholder
text with an appropriate title for your chart.
- `x_axis_label`: This attribute represents the label for the x-axis of the chart. Please replace the
placeholder text with an appropriate label.
- `y_axis_label`: This attribute represents the label for the y-axis of the chart. Please replace the
placeholder text with an appropriate label.
- `data`: This attribute represents the data points for the chart. It is an array of objects, where each
object represents a category or group and its corresponding value.
For a simple example, the template can be filled as follows:

README

Definition of a 'Bar Chart' chart:
A bar chart, also known as a bar graph, is a visualization tool that uses rectangular bars to represent
data. Each bar represents a category or group, and the length or height of the bar corresponds to the
value it represents. Bar charts are commonly used to compare categorical data or show the distribution
of data across different categories.

F.2

Example 2
Template
JSON

README

{ "chart": { "title": "Chart Title", "xAxisLabel": "X-axis Label", "yAxisLabel1": "Y-axis Label 1",
"yAxisLabel2": "Y-axis Label 2", "datasets": [ { "name": "Dataset 1", "type": "line", "data": [] }, {
"name": "Dataset 2", "type": "bar", "data": [] } ] } }
This JSON file template is designed for generating datasets for a 'Multi-axes Line Bar Chart' chart. It includes the following
attributes:
1. chart:
- title: (string) The title of the chart.
- xAxisLabel: (string) The label for the X-axis.
- yAxisLabel1: (string) The label for the Y-axis corresponding to the line chart.
- yAxisLabel2: (string) The label for the Y-axis corresponding to the bar chart.
2. datasets:
- name: (string) The name or label of the dataset.
- type: (string) The type of chart component for the dataset. Can be 'line' or 'bar'.
- data: (array) The array to store the data points for the dataset. Placeholder data should be added here.
The template provides a structure to accommodate both simple and complex examples of a Multi-axes Line Bar Chart. Additional
datasets can be added within the "datasets" array.
Please ensure that the generated data adheres to the structure of this JSON template, including the attribute names and data types, to
ensure consistency when plotting the chart using Python.

Definition of a 'Multi-axes Line Bar Chart':
A multi-axes line bar chart is a type of chart that combines both line and bar charts in a single
visualization. It allows for the comparison of multiple datasets that have different scales or units of
measurement. This chart type uses multiple y-axes, one for each dataset, to display the corresponding
line and bar components.

19

G

Examples for pre-defined topics

Public Policy
Healthcare Systems
Mental Health
Renewable Energy
Water Resources
Manufacturing
Retail Trends
Social Issues
Population Dynamics
Digital Media Consumption
Cryptocurrency and Blockchain
Humanitarian Aid and Development

Gender and Diversity
Economic Development
Artificial Intelligence and Robotics
Consumer Spending Habits
Advertising and Marketing
Cultural Trends
Philanthropy and Nonprofits
International Trade and Commerce
Politics
Business and Finance
Science and Research
Agriculture

Transportation
Weather and Climate
Sports and Recreation
Entertainment and Media
Food and Nutrition
Fashion and Lifestyle
Housing and Real Estate
Travel and Tourism
Crime and Safety
International Relations
Religion and Beliefs
History and Heritage

Labor and Employment
Urban Development
Sustainability and Green Initiatives
Education Policy and Reform
Healthcare Access and Equity
Clean Energy Initiatives
E-commerce Trends
Poverty and Homelessness
Immigration and Migration Patterns
Internet and Social Media Usage
Trends
Decentralized Finance (DeFi)

Trends
Disaster Relief and Emergency Response
LGBTQ+ Rights and Advocacy
Regional Economic Disparities
Automation and Job Displacement
Branding and Brand Loyalty
Subcultural Trends and Movements
Social Entrepreneurship and Impact
Investing
Fair Trade and Ethical Consumption
Import-Export Regulations and Tariffs

We provide a word cloud in the figure above to show the frequency of each word in the defined topic
set. A comprehensive list of all the topics is also provided at the bottom of this figure.

20

H
H.1

Examples of augmented QAs
JSON-only QA: example 1
Chart image

JSON & README
{ "title": "Box Office Revenue Share by Genre",
"description": "This pie chart displays the distribution of box
office revenues among five major movie genres in 2021,
illustrating the diversity in consumer preferences and the strategic
positioning of movie studios.",
"data": [{
"category": "Action Blockbuster",
"value": 350,
"color": "#FF6347"
},
{
"category": "Animated Feature",
"value": 300,
"color": "#3CB371"
},
{
"category": "Gripping Thriller",
"value": 250,
"color": "#4682B4"
},
{
"category": "Sci-Fi Adventure",
"value": 200,
"color": "#DAA520"
},
{
"category": "Historical Drama",
"value": 150,
"color": "#A0522D"
}
]
}

H.2

Title: The title of the pie chart, which gives an idea of the dataset's
overall topic.
Description: A short summary explaining what the pie chart data
represents and any additional information that might be useful to
understand the context.
Data: An array of objects where each object represents a slice of
the pie chart.
- category: A string representing the name of the category this slice
of the pie chart is about.
- value: A numerical value indicating the size of the category in the
dataset. This determines the size of the pie slice.
- color: A string indicating the color of this slice. It can be a HEX
color code or a standard color name.
The JSON template and README file are designed to ensure that
datasets for pie charts can be created with consistent structure and
clarity, accommodating both simple and complex use cases. Make
sure to replace placeholder text and values with actual data relevant
to the pie chart you are creating.

JSON-only QA: example 2
Chart image

JSON & README
{
"chartTitle": "Cybersecurity Domain Risk Analysis Trends",
"datasets": [
{
"label": "2023 Risk Levels",
"data": {
"Network Security": 50,
"Cloud Security": 75,
"Endpoint Security": 30,
"Application Security": 60
}
},
{
"label": "2022 Risk Levels",
"data": {
"Network Security": 50,
"Cloud Security": 65,
"Endpoint Security": 45,
"Application Security": 60
}
}
],
"options": {
"scale": {
"min": 0,
"max": 100,
"stepSize": 10
}
}
}

21

Chart Title: The title of the Radar Chart, which usually describes
the overall data being represented.
Datasets: An array of objects, each representing a different dataset
to be plotted on the Radar Chart.
Dataset Label: A descriptive name for the dataset. This could be the
name of a product, an individual's name, or any other identifier
relevant to the data.
Data: An object containing key-value pairs where the key is the
label for the axis (e.g., 'Communication', 'Battery Life') and the
value is the data point associated with that axis. The number of
axes can vary depending on the use case.
Axis Label: Placeholders for the actual labels of each axis on the
Radar Chart. Replace these placeholders with the appropriate
criteria or variables for your specific use case.
Options: An object containing additional settings and
configurations for the Radar Chart.
Scale: Defines the range and increment of the chart's axes.
Min: The minimum value of the scale (usually 0).
Max: The maximum value of the scale, which should be set
according to the data's range.
StepSize: The interval between values on the scale.

H.3

Data-driven QA: example 1

An example of data extraction QAs
Can you please extract the raw data of this chart?
{ "title": "Seasonal Wildlife Population Dynamics in Yellowstone", "xAxis": { "title":
"Species", "categories": [ "Elk", "Bison", "Wolves", "Bears" ] }, "yAxis": { "title": "Seasons",
"categories": [ "Spring", "Summer", "Autumn", "Winter" ] }, "colorScale": { "minValue": 0,
"maxValue": 100, "startColor": "#FFFFFF", "endColor": "#FF6347" }, "data": [ { "x": 0, "y": 0,
"value": 40 }, { "x": 0, "y": 1, "value": 70 }, { "x": 0, "y": 2, "value": 60 }, { "x": 0, "y": 3, "value": 30
}, { "x": 1, "y": 0, "value": 50 }, { "x": 1, "y": 1, "value": 80 }, { "x": 1, "y": 2, "value": 55 }, { "x": 1, "y":
3, "value": 35 }, { "x": 2, "y": 0, "value": 20 }, { "x": 2, "y": 1, "value": 60 }, { "x": 2, "y": 2, "value":
45 }, { "x": 2, "y": 3, "value": 25 }, { "x": 3, "y": 0, "value": 30 }, { "x": 3, "y": 1, "value": 50 }, { "x":
3, "y": 2, "value": 40 }, { "x": 3, "y": 3, "value": 20 } ] }
Which season generally shows the highest population densities across all species?

H.4

Long answer

Reviewing the heatmap data, Summer has high values for all species:
Elk (70), Bison (80), Wolves (60), and Bears (50). Thus, Summer generally
shows the highest population densities.

Short answer

Summer

Data-driven QA: example 2

An example of data extraction QAs
Can you please extract the raw data of this chart?
{ "chart": { "title": "Disaster Response Stages Efﬁciency", "description": "This funnel chart
maps the stages of disaster response from the initial alert to recovery, highlighting the
number of actions completed and the conversion rates between each stage. The cyclical
trend observed suggests repeated challenges in transitioning efﬁciently from one stage
to the next." }, "stages": [ { "stage": "Early Warning and Alert", "value": 1000,
"conversionRate": "85%" }, { "stage": "Immediate Response", "value": 850, "conversionRate":
"75%" }, { "stage": "Assessment", "value": 638, "conversionRate": "60%" }, { "stage": "Relief
Distribution", "value": 383, "conversionRate": "80%" }, { "stage": "Recovery and
Rehabilitation", "value": 306, "conversionRate": null } ] }
What is the sum of conversion rates for all stages where it is mentioned?

Long answer

The sum of conversion rates for all stages mentioned is calculated by
adding the rates: 85% (Early Warning and Alert) + 75% (Immediate
Response) + 60% (Assessment) + 80% (Relief Distribution) = 300%.

Short answer

300%

22

I
I.1

Examples of data prompting
Example 1

Without data extraction QA in the inference
What is the acceptance rate of manuscripts that have been revised?
50%

With data extraction QA in the inference
Retrieve the raw data from the image and organize it into a JSON dictionary.
{'chart': {'title': 'Research Paper Publication Funnel', 'xAxisLabel': 'Stage', 'yAxisLabel':
'Number of Papers'}, 'data': [{'stage': 'Submitted Manuscripts', 'count': 1200}, {'stage':
'Passed Initial Screening', 'count': 900}, {'stage': 'Peer Review Completed', 'count': 700},
{'stage': 'Revisions Submitted', 'count': 500}, {'stage': 'Accepted for Publication', 'count':
300}]}
What is the acceptance rate of manuscripts that have been revised?
60%

I.2

Example 2

Without data extraction QA in the inference
What proportion of the total distribution is made up by 'Sustainable
Energy' and 'Healthcare Innovations' combined?
45%

With data extraction QA in the inference
Retrieve the raw data from the image and organize it into a JSON dictionary.
{'chart_title': 'Impact of Social Entrepreneurship Sectors on Investment Returns',
'x_axis_label': 'Sectors', 'y_axis_label_pie': 'Distribution of Sectors', 'y_axis_label_bar':
'Average Investment Return (%)', 'data': [{'category': 'Healthcare Innovation', 'value_pie':
35, 'value_bar': 12}, {'category': 'Sustainable Energy', 'value_pie': 25, 'value_bar': 15},
{'category': 'Education Technology', 'value_pie': 20, 'value_bar': 10}, {'category':
'Agricultural Development', 'value_pie': 15, 'value_bar': 8}, {'category': 'Financial Inclusion',
'value_pie': 5, 'value_bar': 20}]}
What proportion of the total distribution is made up by 'Sustainable
Energy' and 'Healthcare Innovations' combined?
60%

23

J

Examples from our benchmark

J.1

Example 1

JSON Data
{'chart': {'type': 'donut', 'title': 'AI & Robotics Investment Distribution 2023'}, 'data': {'labels': ['Machine
Learning', 'Natural Language Processing', 'Robotics', 'Computer Vision', 'Speech Recognition'], 'datasets':
[{'label': 'Investment Proportions', 'data': [35, 25, 15, 15, 10], 'backgroundColor': ['#6495ED', '#FFD700',
'#DC143C', '#32CD32', '#FF8C00']}]}}
Literal Question
Question:
Long Answer:

Short Answer:

How much less investment did Speech Recognition receive compared to Natural
Language Processing?
Speech Recognition received 15% less investment than Natural Language
Processing, with Speech Recognition at 10% and Natural Language Processing
at 25%.
15%

Inferential Question
Question:
Long Answer:
Short Answer:

What two sectors together make up half of the total investment?
Machine Learning and Natural Language Processing together make up half of
the total investment, with percentages of 35% and 25% respectively.
Machine Learning and Natural Language Processing

Reasoning Question
Question:
Long Answer:

Short Answer:

What fraction of the total investment is allocated to fields other than Machine
Learning?
Fields other than Machine Learning receive a combined total of 65% of the
investment, which is equivalent to the fraction 65/100 or 13/20 of the total
investment.
13/20

Example 1

24

J.2

Example 2

JSON Data
{'chart': {'title': 'Quarterly Revenue and Profit Comparison', 'xAxisLabel': 'Quarter', 'yAxisLabel': 'Amount (in
million USD)'}, 'data': [{'category': 'Revenue', 'values': [{'x': 'Q1 2021', 'y': 120}, {'x': 'Q2 2021', 'y': 150}, {'x':
'Q3 2021', 'y': 130}, {'x': 'Q4 2021', 'y': 170}]}, {'category': 'Profit', 'values': [{'x': 'Q1 2021', 'y': 30}, {'x': 'Q2
2021', 'y': 50}, {'x': 'Q3 2021', 'y': 40}, {'x': 'Q4 2021', 'y': 60}]}]}
Literal Question
Question:
Long Answer:
Short Answer:

What was the Revenue in Q3 2021?
In Q3 2021, the Revenue was 130 million USD as shown on the chart.
130 million USD

Inferential Question
Question:
Long Answer:

Short Answer:

Which quarter had the highest ratio of Profit to Revenue?
To determine the highest ratio of Profit to Revenue, we compare the ratios for
each quarter. The highest ratio is in Q2 2021, with Profit at 50 million USD and
Revenue at 150 million USD, giving a ratio of 1:3.
Q2 2021

Reasoning Question
Question:
Long Answer:
Short Answer:

If Profit in Q1 2022 is expected to be 20% higher than Q4 2021, what would be
the expected Profit?
If Profit in Q1 2022 is expected to be 20% higher than Q4 2021's 60 million
USD, the expected Profit would be 60 * 1.20, which is 72 million USD.
72 million USD

Example 2

25

J.3

Example 3

JSON Data
{'chart': {'title': 'Trends in Poverty and Homelessness Rates in Metropolis Over a Decade', 'xAxisLabel': 'Year',
'yAxisLabel': 'Percentage (%)'}, 'data': [{'category': 'Poverty Rate', 'values': [{'x': '2010', 'y': 15.0}, {'x': '2011',
'y': 15.5}, {'x': '2012', 'y': 16.0}, {'x': '2013', 'y': 15.8}, {'x': '2014', 'y': 15.6}, {'x': '2015', 'y': 14.7}, {'x': '2016',
'y': 14.0}, {'x': '2017', 'y': 13.5}, {'x': '2018', 'y': 13.0}, {'x': '2019', 'y': 12.5}, {'x': '2020', 'y': 14.0}]},
{'category': 'Homelessness Rate', 'values': [{'x': '2010', 'y': 0.9}, {'x': '2011', 'y': 1.0}, {'x': '2012', 'y': 1.1}, {'x':
'2013', 'y': 1.2}, {'x': '2014', 'y': 1.3}, {'x': '2015', 'y': 1.2}, {'x': '2016', 'y': 1.1}, {'x': '2017', 'y': 1.0}, {'x': '2018',
'y': 0.8}, {'x': '2019', 'y': 0.7}, {'x': '2020', 'y': 0.9}]}, {'category': 'Extreme Poverty Rate', 'values': [{'x': '2010',
'y': 2.0}, {'x': '2011', 'y': 2.2}, {'x': '2012', 'y': 2.4}, {'x': '2013', 'y': 2.5}, {'x': '2014', 'y': 2.3}, {'x': '2015', 'y':
2.1}, {'x': '2016', 'y': 2.0}, {'x': '2017', 'y': 1.9}, {'x': '2018', 'y': 1.7}, {'x': '2019', 'y': 1.5}, {'x': '2020', 'y':
1.8}]}]}
Literal Question
Question:
Long Answer:
Short Answer:

In which year did the Poverty Rate reach its lowest value?
According to the chart data, the Poverty Rate reached its lowest value in 2019 at
12.5%.
2019

Inferential Question
Question:
Long Answer:

Short Answer:

Did any category show a consistent decline over the entire decade?
No category showed a consistent decline over the entire decade. While Poverty
Rate and Extreme Poverty Rate generally declined until 2019, they both
increased in 2020, and the Homelessness Rate fluctuated throughout the decade.
No

Reasoning Question
Question:
Long Answer:

Short Answer:

What is the average annual decrease in the Poverty Rate from 2010 to 2019?
From 2010 to 2019, the Poverty Rate decreased from 15.0% to 12.5%. This is a
total decrease of 2.5 percentage points over 9 years, which gives an average
annual decrease of about 0.278 percentage points per year.
Approximately 0.278 percentage points per year

Example 32

26

