{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification,BertModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, directory, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Example: Read each file and assign a label based on the filename or content\n",
    "        for filename in os.listdir(directory):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read().strip()\n",
    "                self.texts.append(text)\n",
    "                # Assuming labels are somehow derived from filenames or content\n",
    "                self.labels.append(int(filename[0]))  # Simplistic example\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(text, max_length=self.max_len, padding='max_length', truncation=True)\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "model = BertModel.from_pretrained('bert-base-cased', output_attentions=True)\n",
    "# Create dataset\n",
    "dataset = TextDataset(tokenizer, '/home/angel/universe/kv_version/dataset/')\n",
    "\n",
    "# You can now pass 'dataset' to the Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/angel/universe/test_trainer/tokenizer_config.json',\n",
       " '/home/angel/universe/test_trainer/special_tokens_map.json',\n",
       " '/home/angel/universe/test_trainer/vocab.txt',\n",
       " '/home/angel/universe/test_trainer/added_tokens.json',\n",
       " '/home/angel/universe/test_trainer/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Specify the path where you want to save the model\n",
    "save_directory = \"/home/angel/universe/test_trainer\"\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(save_directory)\n",
    "\n",
    "# Optionally save the tokenizer associated with your model\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure the save_directory is defined correctly\n",
    "save_directory = \"/home/angel/universe/test_trainer\"\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "max_length = 512\n",
    "chunk_size = 400  # A bit less than max_length to allow for special tokens\n",
    "stride = 50  # Overlapping tokens to avoid loss of context\n",
    "\n",
    "# Read the file\n",
    "file_path = \"/home/angel/universe/kv_version/dataset/1_llm.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "# Tokenize the input text with chunking for long sequences\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "\n",
    "# Get model outputs, including attentions\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions  # Tuple of attention tensors for each layer\n",
    "\n",
    "# Choose a specific layer and head to visualize\n",
    "layer_index = -5  # Middle layer\n",
    "head_index = 0  # First head\n",
    "\n",
    "# Extract the attention matrix for the specified layer and head\n",
    "attention = attentions[layer_index][0, head_index].detach().cpu().numpy()\n",
    "attention = attention[:,1:]\n",
    "\n",
    "# Convert token IDs back to tokens for labeling\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention between 'existing' and 'We': 0.99\n",
      "Attention between 'Han' and 'We': 0.99\n",
      "Attention between 'Fe' and 'We': 0.99\n",
      "Attention between 'method' and 'We': 0.98\n",
      "Attention between 'Self' and 'We': 0.98\n",
      "Attention between 'motivation' and 'We': 0.97\n",
      "Attention between 'studies' and 'We': 0.97\n",
      "Attention between 'LL' and 'We': 0.97\n",
      "Attention between 'LL' and 'We': 0.97\n",
      "Attention between 'hall' and 'We': 0.97\n",
      "Attention between 'sampling' and 'We': 0.97\n",
      "Attention between 'reasoning' and 'We': 0.97\n",
      "Attention between 'explanations' and 'We': 0.95\n",
      "Attention between 'modules' and 'We': 0.95\n",
      "Attention between 'Internal' and 'We': 0.94\n",
      "Attention between 'reasoning' and 'We': 0.94\n",
      "Attention between 'yet' and 'stream': 0.94\n",
      "Attention between 'phenomena' and 'We': 0.93\n",
      "Attention between 'stream' and 'We': 0.93\n",
      "Attention between 'theoretical' and 'We': 0.93\n",
      "Attention between 'or' and 'We': 0.93\n",
      "Attention between 'accurately' and 'We': 0.93\n",
      "Attention between 'two' and 'We': 0.93\n",
      "Attention between 'Qing' and 'We': 0.92\n",
      "Attention between 'Shi' and 'We': 0.92\n",
      "Attention between 'Ron' and 'We': 0.92\n",
      "Attention between 'paper' and 'We': 0.91\n",
      "Attention between 'unified' and 'We': 0.91\n",
      "Attention between 'works' and 'We': 0.90\n",
      "Attention between 'layer' and 'late': 0.90\n",
      "Attention between 'NO' and '14': 0.90\n",
      "Attention between 'mining' and 'We': 0.90\n",
      "Attention between 'de' and 'We': 0.90\n",
      "Attention between 'Self' and 'We': 0.89\n",
      "Attention between 'unified' and 'We': 0.89\n",
      "Attention between 'Yu' and 'Qing': 0.88\n",
      "Attention between 'LL' and 'We': 0.88\n",
      "Attention between 'response' and 'We': 0.87\n",
      "Attention between 'former' and 'We': 0.87\n",
      "Attention between 'framework' and 'We': 0.87\n",
      "Attention between 'Internal' and 'We': 0.86\n",
      "Attention between 'Internal' and 'We': 0.86\n",
      "Attention between 'def' and 'We': 0.86\n",
      "Attention between 'Self' and 'We': 0.86\n",
      "Attention between 'these' and 'We': 0.86\n",
      "Attention between 'framework' and 'We': 0.84\n",
      "Attention between 'issues' and 'We': 0.84\n",
      "Attention between 'framework' and 'We': 0.84\n",
      "Attention between 'surveys' and 'We': 0.83\n",
      "Attention between 'IEEE' and 'We': 0.82\n",
      "Attention between 'Con' and 'captures': 0.82\n",
      "Attention between 'Con' and 'mining': 0.81\n",
      "Attention between 'Xu' and 'We': 0.81\n",
      "Attention between 'expected' and 'We': 0.81\n",
      "Attention between 'Internal' and 'We': 0.80\n",
      "Attention between 'Fe' and 'named': 0.78\n",
      "Attention between 'sum' and 'We': 0.78\n",
      "Attention between 'Nonetheless' and 'We': 0.77\n",
      "Attention between 'lack' and 'these': 0.77\n",
      "Attention between 'focus' and 'existing': 0.77\n",
      "Attention between 'these' and 'We': 0.76\n",
      "Attention between 'Internal' and 'We': 0.76\n",
      "Attention between 'Liang' and 'We': 0.75\n",
      "Attention between '[SEP]' and 'We': 0.75\n",
      "Attention between 'leverage' and 'We': 0.75\n",
      "Attention between 'hall' and 'We': 0.75\n",
      "Attention between 'this' and 'We': 0.75\n",
      "Attention between 'Ex' and 'We': 0.73\n",
      "Attention between 'Con' and 'termed': 0.73\n",
      "Attention between 'while' and 'We': 0.73\n",
      "Attention between 'co' and 'We': 0.73\n",
      "Attention between 'these' and 'We': 0.72\n",
      "Attention between 'theoretical' and 'We': 0.72\n",
      "Attention between 'efforts' and 'We': 0.70\n",
      "Attention between 'predominantly' and 'existing': 0.70\n",
      "Attention between 'Wang' and 'We': 0.69\n",
      "Attention between 'Hu' and 'Ron': 0.69\n",
      "Attention between 'common' and 'We': 0.69\n",
      "Attention between 'Song' and 'We': 0.69\n",
      "Attention between 'framework' and 'We': 0.69\n",
      "Attention between 'Large' and 'We': 0.69\n",
      "Attention between 'based' and 'We': 0.69\n",
      "Attention between 'Self' and 'We': 0.68\n",
      "Attention between 'prefix' and 'We': 0.68\n",
      "Attention between 'Li' and 'Hu': 0.68\n",
      "Attention between 'late' and 'We': 0.67\n",
      "Attention between 'presence' and 'We': 0.67\n",
      "Attention between 'Self' and 'We': 0.66\n",
      "Attention between 'Language' and 'We': 0.65\n",
      "Attention between 'Self' and 'We': 0.64\n",
      "Attention between 'language' and 'We': 0.62\n",
      "Attention between 'respond' and 'We': 0.61\n",
      "Attention between 'address' and 'We': 0.61\n",
      "Attention between 'lack' and 'We': 0.60\n",
      "Attention between 'evaluating' and 'We': 0.60\n",
      "Attention between 'we' and 'We': 0.59\n",
      "Attention between 'effective' and 'We': 0.56\n",
      "Attention between 'Li' and 'We': 0.56\n",
      "Attention between '202' and 'We': 0.56\n",
      "Attention between 'consists' and 'We': 0.55\n",
      "Attention between 'perspective' and 'We': 0.55\n",
      "Attention between 'capable' and 'We': 0.55\n",
      "Attention between 'Xu' and '202': 0.54\n",
      "Attention between 'termed' and 'framework': 0.53\n",
      "Attention between 'initiated' and 'We': 0.53\n",
      "Attention between 'models' and 'We': 0.51\n",
      "Attention between 'latter' and 'We': 0.51\n",
      "Attention between 'They' and 'We': 0.51\n",
      "Attention between 'exhibit' and 'accurately': 0.51\n",
      "Attention between 'Li' and 'We': 0.48\n",
      "Attention between 'captures' and 'We': 0.48\n",
      "Attention between 'Reason' and 'We': 0.48\n",
      "Attention between 'Models' and 'Large': 0.48\n",
      "Attention between 'are' and 'We': 0.47\n",
      "Attention between 'generate' and 'We': 0.46\n",
      "Attention between 'Zheng' and 'We': 0.46\n",
      "Attention between 'cat' and 'We': 0.45\n",
      "Attention between '202' and 'We': 0.44\n",
      "Attention between 'Models' and 'We': 0.42\n",
      "Attention between 'examining' and 'We': 0.41\n",
      "Attention between 'have' and 'We': 0.41\n",
      "Attention between 'often' and 'We': 0.41\n",
      "Attention between 'Self' and 'We': 0.39\n",
      "Attention between 'Xu' and 'We': 0.39\n",
      "Attention between 'Self' and 'We': 0.39\n",
      "Attention between 'such' and 'We': 0.39\n",
      "Attention between 'evaluating' and 'LL': 0.36\n",
      "Attention between 'we' and 'We': 0.36\n",
      "Attention between 'Xi' and 'We': 0.35\n",
      "Attention between '19' and 'We': 0.35\n",
      "Attention between 'latter' and 'while': 0.35\n",
      "Attention between 'capable' and 'theoretical': 0.34\n",
      "Attention between 'we' and 'framework': 0.33\n",
      "Attention between 'models' and 'Large': 0.32\n",
      "Attention between 'Con' and 'We': 0.32\n",
      "Attention between 'LA' and 'We': 0.31\n",
      "Attention between 'introduce' and 'We': 0.30\n",
      "Attention between 'sum' and 'We': 0.29\n",
      "Attention between 'often' and 'accurately': 0.28\n",
      "Attention between 'itself' and 'LL': 0.28\n",
      "Attention between 'Wang' and 'Han': 0.28\n",
      "Attention between 'They' and 'initiated': 0.27\n",
      "Attention between 'based' and 'response': 0.27\n",
      "Attention between 'Con' and 'Self': 0.26\n",
      "Attention between '[CLS]' and 'We': 0.26\n",
      "Attention between 'Li' and 'We': 0.26\n",
      "Attention between 'Senior' and 'We': 0.25\n",
      "Attention between 'have' and 'Re': 0.25\n",
      "Attention between 'Internal' and 'We': 0.24\n",
      "Attention between 'Con' and 'We': 0.23\n",
      "Attention between 'layer' and 'We': 0.22\n",
      "Attention between 'Large' and 'We': 0.22\n",
      "Attention between 'Para' and 'We': 0.22\n",
      "Attention between 'Does' and 'We': 0.22\n",
      "Attention between 'mit' and 'We': 0.22\n",
      "Attention between 'content' and 'We': 0.21\n",
      "Attention between 'mit' and 'itself': 0.21\n",
      "Attention between 'Liang' and 'Xu': 0.21\n",
      "Attention between '240' and 'We': 0.21\n",
      "Attention between 'Survey' and 'We': 0.20\n",
      "Attention between 'This' and 'We': 0.20\n",
      "Attention between 'Nonetheless' and 'issues': 0.20\n",
      "Attention between 'itself' and 'We': 0.20\n",
      "Attention between 'which' and 'We': 0.19\n",
      "Attention between 'Co' and 'We': 0.19\n",
      "Attention between 'we' and 'paper': 0.19\n",
      "Attention between 'predominantly' and 'We': 0.19\n",
      "Attention between 'layer' and 'We': 0.18\n",
      "Attention between 'Evolution' and 'We': 0.18\n",
      "Attention between 'Internal' and 'We': 0.18\n",
      "Attention between 'Self' and 'We': 0.18\n",
      "Attention between 'termed' and 'We': 0.17\n",
      "Attention between 'cat' and 'existing': 0.17\n",
      "Attention between 'been' and 'Re': 0.17\n",
      "Attention between 'cat' and 'focus': 0.17\n",
      "Attention between 'which' and 'theoretical': 0.17\n",
      "Attention between 'while' and 'signals': 0.16\n",
      "Attention between 'focus' and 'We': 0.16\n",
      "Attention between 'offers' and 'We': 0.15\n",
      "Attention between 'Work' and 'We': 0.15\n",
      "Attention between 'such' and 'We': 0.15\n",
      "Attention between 'Furthermore' and 'We': 0.15\n",
      "Attention between 'Self' and 'Self': 0.15\n",
      "Attention between 'generate' and 'reasoning': 0.15\n",
      "Attention between 'named' and 'We': 0.15\n",
      "Attention between '202' and '19': 0.14\n",
      "Attention between 'are' and 'language': 0.14\n",
      "Attention between 'Self' and 'Evaluation': 0.14\n",
      "Attention between 'Almost' and 'We': 0.13\n",
      "Attention between 'Con' and 'We': 0.13\n",
      "Attention between 'Fe' and 'We': 0.13\n",
      "Attention between 'Evaluation' and 'Self': 0.13\n",
      "Attention between 'Late' and 'We': 0.13\n",
      "Attention between 'share' and 'We': 0.13\n",
      "Attention between 'Member' and 'We': 0.12\n",
      "Attention between 'Li' and 'Xu': 0.11\n",
      "Attention between 'initiated' and 'have': 0.11\n",
      "Attention between 'termed' and 'theoretical': 0.11\n",
      "Attention between 'Jul' and 'We': 0.11\n",
      "Attention between 'sum' and 'We': 0.11\n",
      "Attention between 'such' and 'unified': 0.10\n",
      "Attention between 'been' and 'We': 0.10\n",
      "Attention between 'concern' and 'critical': 0.10\n",
      "Attention between 'lack' and 'We': 0.10\n",
      "Attention between 'involving' and 'We': 0.10\n",
      "Attention between 'Hu' and 'We': 0.09\n",
      "Attention between 'IEEE' and 'Member': 0.09\n",
      "Attention between 'initiated' and 'Re': 0.09\n",
      "Attention between 'Con' and 'We': 0.09\n",
      "Attention between 'we' and 'We': 0.09\n",
      "Attention between 'examining' and 'existing': 0.09\n",
      "Attention between 'promising' and 'been': 0.09\n",
      "Attention between 'relevant' and 'critical': 0.09\n",
      "Attention between 'Fe' and 'captures': 0.08\n",
      "Attention between 'Fe' and 'Self': 0.08\n",
      "Attention between 'layer' and 'late': 0.08\n",
      "Attention between 'mit' and 'LL': 0.08\n",
      "Attention between 'these' and 'issues': 0.08\n",
      "Attention between 'framework' and 'We': 0.08\n",
      "Attention between 'Self' and '14': 0.08\n",
      "Attention between 'This' and 'Self': 0.08\n",
      "Attention between 'unified' and 'which': 0.08\n",
      "Attention between 'are' and 'LL': 0.08\n",
      "Attention between 'promising' and 'critical': 0.08\n",
      "Attention between 'Con' and 'We': 0.08\n",
      "Attention between 'Con' and 'We': 0.07\n",
      "Attention between '14' and 'We': 0.07\n",
      "Attention between 'class' and 'We': 0.07\n",
      "Attention between 'response' and 'Late': 0.07\n",
      "Attention between 'studies' and 'critical': 0.07\n",
      "Attention between 'promising' and 'future': 0.07\n",
      "Attention between 'or' and 'these': 0.07\n",
      "Attention between 'Re' and 'Self': 0.07\n",
      "Attention between 'these' and 'address': 0.07\n",
      "Attention between 'signals' and 'Large': 0.07\n",
      "Attention between 'exhibit' and 'We': 0.07\n",
      "Attention between '145' and 'We': 0.07\n",
      "Attention between 'theoretical' and 'yet': 0.07\n",
      "Attention between 'Really' and 'We': 0.07\n",
      "Attention between 'class' and 'been': 0.07\n",
      "Attention between 'research' and 'critical': 0.07\n",
      "Attention between '145' and '240': 0.07\n",
      "Attention between 'tasks' and 'outline': 0.07\n",
      "Attention between 'studies' and 'outline': 0.06\n",
      "Attention between 'common' and 'share': 0.06\n",
      "Attention between 'studies' and 'critical': 0.06\n",
      "Attention between 'Fe' and 'Self': 0.06\n",
      "Attention between 'this' and 'works': 0.06\n",
      "Attention between 'these' and 'content': 0.06\n",
      "Attention between 'been' and 'been': 0.06\n",
      "Attention between 'critical' and 'We': 0.06\n",
      "Attention between 'cat' and 'LL': 0.06\n",
      "Attention between 'we' and 'outline': 0.06\n",
      "Attention between 'numerous' and 'been': 0.06\n",
      "Attention between '[CLS]' and 'Work': 0.06\n",
      "Attention between 'NO' and 'We': 0.06\n",
      "Attention between 'framework' and 'effective': 0.06\n",
      "Attention between 'Shi' and 'IEEE': 0.06\n",
      "Attention between 'numerous' and 'numerous': 0.06\n",
      "Attention between 'presence' and 'reasoning': 0.06\n",
      "Attention between 'Self' and 'Self': 0.06\n",
      "Attention between 'viewpoint' and 'critical': 0.06\n",
      "Attention between '[CLS]' and 'itself': 0.06\n",
      "Attention between 'Yu' and 'We': 0.06\n",
      "Attention between 'or' and 'numerous': 0.06\n",
      "Attention between 'future' and 'critical': 0.06\n",
      "Attention between 'including' and 'numerous': 0.06\n",
      "Attention between 'work' and 'critical': 0.06\n",
      "Attention between 'Fe' and 'Self': 0.06\n",
      "Attention between 'Xu' and 'Yu': 0.05\n",
      "Attention between 'Ex' and 'We': 0.05\n",
      "Attention between 'framework' and 'Late': 0.05\n",
      "Attention between 'We' and 'We': 0.05\n",
      "Attention between 'layer' and 'We': 0.05\n",
      "Attention between 'sum' and 'perspective': 0.05\n",
      "Attention between 'systematically' and 'been': 0.05\n",
      "Attention between 'Con' and 'research': 0.05\n",
      "Attention between 'including' and 'future': 0.05\n",
      "Attention between 'Con' and 'studies': 0.05\n",
      "Attention between 'Self' and 'such': 0.05\n",
      "Attention between 'offers' and 'which': 0.05\n",
      "Attention between '[CLS]' and 'studies': 0.05\n",
      "Attention between 'studies' and 'outline': 0.05\n",
      "Attention between 'either' and 'been': 0.05\n",
      "Attention between 'del' and 'future': 0.05\n",
      "Attention between 'methods' and 'Late': 0.05\n",
      "Attention between 'exhibit' and 'expected': 0.05\n",
      "Attention between 'several' and 'numerous': 0.05\n",
      "Attention between 'including' and 'these': 0.05\n",
      "Attention between 'we' and 'works': 0.05\n",
      "Attention between 'Con' and 'research': 0.05\n",
      "Attention between 'Con' and 'studies': 0.05\n",
      "Attention between 'itself' and 'del': 0.05\n",
      "Attention between 'theoretical' and 'stream': 0.05\n",
      "Attention between 'directions' and 'Late': 0.05\n",
      "Attention between 'concern' and 'future': 0.05\n",
      "Attention between 'We' and 'We': 0.05\n",
      "Attention between 'sum' and 'these': 0.05\n",
      "Attention between 'AU' and 'We': 0.05\n",
      "Attention between 'been' and 'have': 0.05\n",
      "Attention between 'assess' and 'We': 0.05\n",
      "Attention between 'Li' and 'Xu': 0.05\n",
      "Attention between 'critical' and 'been': 0.05\n",
      "Attention between 'Furthermore' and 'hypothesis': 0.05\n",
      "Attention between 'Self' and 'Self': 0.05\n",
      "Attention between 'several' and 'We': 0.05\n",
      "Attention between 'Hour' and 'We': 0.05\n",
      "Attention between 'signals' and 'We': 0.05\n",
      "Attention between 'Hu' and 'Hu': 0.04\n",
      "Attention between 'directions' and 'outline': 0.04\n",
      "Attention between 'mining' and 'capable': 0.04\n",
      "Attention between 'methods' and 'We': 0.04\n",
      "Attention between 'several' and 'been': 0.04\n",
      "Attention between 'share' and 'initiated': 0.04\n",
      "Attention between 'Almost' and 'studies': 0.04\n",
      "Attention between 'involving' and 'LL': 0.04\n",
      "Attention between 'We' and 'Self': 0.04\n",
      "Attention between 'We' and 'We': 0.04\n",
      "Attention between 'Internal' and 'former': 0.04\n",
      "Attention between 'we' and 'we': 0.04\n",
      "Attention between 'cat' and 'surveys': 0.04\n",
      "Attention between 'We' and 'Self': 0.04\n",
      "Attention between 'layer' and 'late': 0.04\n",
      "Attention between 'or' and 'future': 0.04\n",
      "Attention between 'critical' and 'critical': 0.04\n",
      "Attention between 'Hour' and 'Ex': 0.04\n",
      "Attention between 'enhance' and 'Reason': 0.04\n",
      "Attention between 'tasks' and 'critical': 0.04\n",
      "Attention between 'these' and 'concern': 0.04\n",
      "Attention between 'focus' and 'surveys': 0.04\n",
      "Attention between 'examining' and 'focus': 0.04\n",
      "Attention between 'have' and 'address': 0.04\n",
      "Attention between 'or' and 'outline': 0.04\n",
      "Attention between 'We' and 'Self': 0.04\n",
      "Attention between 'evaluation' and 'model': 0.04\n",
      "Attention between 'Furthermore' and 'itself': 0.04\n",
      "Attention between 'have' and 'been': 0.04\n",
      "Attention between 'often' and 'expected': 0.04\n",
      "Attention between 'several' and 'Para': 0.04\n",
      "Attention between 'initiated' and 'Self': 0.04\n",
      "Attention between 'Hour' and 'tasks': 0.04\n",
      "Attention between 'relevant' and 'numerous': 0.04\n",
      "Attention between 'numerous' and 'We': 0.04\n",
      "Attention between 'Almost' and 'research': 0.04\n",
      "Attention between 'research' and 'been': 0.04\n",
      "Attention between 'which' and 'framework': 0.04\n",
      "Attention between 'been' and 'address': 0.04\n",
      "Attention between 'numerous' and 'evaluation': 0.04\n",
      "Attention between 'sum' and 'critical': 0.04\n",
      "Attention between 'Really' and 'tasks': 0.04\n",
      "Attention between 'including' and 'We': 0.04\n",
      "Attention between 'itself' and 'involving': 0.04\n",
      "Attention between 'numerous' and 'these': 0.04\n",
      "Attention between 'Does' and 'hypothesis': 0.04\n",
      "Attention between 'reasoning' and 'lack': 0.04\n",
      "Attention between 'Self' and '14': 0.04\n",
      "Attention between 'several' and 'these': 0.04\n",
      "Attention between 'Really' and 'research': 0.04\n",
      "Attention between 'lines' and 'critical': 0.04\n",
      "Attention between 'sum' and 'these': 0.04\n",
      "Attention between 'itself' and 'Late': 0.04\n",
      "Attention between 'Furthermore' and 'research': 0.04\n",
      "Attention between 'evaluation' and 'critical': 0.04\n",
      "Attention between 'class' and 'del': 0.04\n",
      "Attention between 'employed' and 'Does': 0.03\n",
      "Attention between 'these' and 'we': 0.03\n",
      "Attention between 'bench' and 'critical': 0.03\n",
      "Attention between 'directions' and 'several': 0.03\n",
      "Attention between 'theoretical' and 'effective': 0.03\n",
      "Attention between 'Self' and 'Self': 0.03\n",
      "Attention between 'we' and 'Work': 0.03\n",
      "Attention between 'Self' and 'Self': 0.03\n",
      "Attention between 'Late' and 'work': 0.03\n",
      "Attention between 'enhance' and 'critical': 0.03\n",
      "Attention between 'hypothesis' and 'Late': 0.03\n",
      "Attention between 'predominantly' and 'predominantly': 0.03\n",
      "Attention between 'Con' and 'We': 0.03\n",
      "Attention between 'numerous' and 'future': 0.03\n",
      "Attention between 'Con' and 'itself': 0.03\n",
      "Attention between '[CLS]' and 'research': 0.03\n",
      "Attention between 'lines' and 'Late': 0.03\n",
      "Attention between 'including' and 'critical': 0.03\n",
      "Attention between 'respond' and 'expected': 0.03\n",
      "Attention between 'Almost' and 'itself': 0.03\n",
      "Attention between 'perspective' and 'these': 0.03\n",
      "Attention between 'such' and 'phenomena': 0.03\n",
      "Attention between 'outline' and 'tasks': 0.03\n",
      "Attention between 'bench' and 'future': 0.03\n",
      "Attention between 'these' and 'We': 0.03\n",
      "Attention between 'class' and 'critical': 0.03\n",
      "Attention between 'We' and 'propose': 0.03\n",
      "Attention between 'bench' and 'numerous': 0.03\n",
      "Attention between 'signals' and 'critical': 0.03\n",
      "Attention between 'signals' and 'We': 0.03\n",
      "Attention between 'Furthermore' and 'Work': 0.03\n",
      "Attention between 'unified' and 'efforts': 0.03\n",
      "Attention between 'Con' and 'itself': 0.03\n",
      "Attention between 'Furthermore' and 'studies': 0.03\n",
      "Attention between 'Ex' and 'tasks': 0.03\n",
      "Attention between 'numerous' and 'critical': 0.03\n",
      "Attention between 'or' and 'Hour': 0.03\n",
      "Attention between 'been' and 'initiated': 0.03\n",
      "Attention between 'content' and 'generate': 0.03\n",
      "Attention between 'Ex' and 'studies': 0.03\n",
      "Attention between 'relevant' and 'evaluation': 0.03\n",
      "Attention between 'leverage' and 'while': 0.03\n",
      "Attention between 'Internal' and 'critical': 0.03\n",
      "Attention between 'future' and 'We': 0.03\n",
      "Attention between 'Does' and 'Furthermore': 0.03\n",
      "Attention between 'framework' and 'several': 0.03\n",
      "Attention between 'outline' and 'concern': 0.03\n",
      "Attention between 'outline' and 'critical': 0.03\n",
      "Attention between 'viewpoint' and 'We': 0.03\n",
      "Attention between 'Evaluation' and 'We': 0.03\n",
      "Attention between 'explanations' and 'offers': 0.03\n",
      "Attention between 'del' and 'We': 0.03\n",
      "Attention between 'perspective' and 'unified': 0.03\n",
      "Attention between 'Furthermore' and 'Furthermore': 0.03\n",
      "Attention between 'Ex' and 'research': 0.03\n",
      "Attention between 'these' and 'numerous': 0.03\n",
      "Attention between 'has' and 'future': 0.03\n",
      "Attention between 'relevant' and 'these': 0.03\n",
      "Attention between 'these' and 'these': 0.03\n",
      "Attention between 'enhance' and 'future': 0.03\n",
      "Attention between 'concern' and 'Does': 0.03\n",
      "Attention between 'often' and 'latter': 0.03\n",
      "Attention between 'this' and 'paper': 0.03\n",
      "Attention between 'Ex' and 'concern': 0.03\n",
      "Attention between 'address' and 'content': 0.03\n",
      "Attention between 'Really' and 'studies': 0.03\n",
      "Attention between 'Really' and 'propose': 0.03\n",
      "Attention between 'Con' and 'mining': 0.03\n",
      "Attention between 'critical' and 'future': 0.03\n",
      "Attention between 'outline' and 'future': 0.03\n",
      "Attention between 'relevant' and 'future': 0.03\n",
      "Attention between 'response' and 'Ex': 0.03\n",
      "Attention between 'This' and 'hypothesis': 0.03\n",
      "Attention between 'mit' and 'Self': 0.03\n",
      "Attention between 'perspective' and 'efforts': 0.03\n",
      "Attention between 'promising' and 'We': 0.03\n",
      "Attention between 'these' and 'Para': 0.03\n",
      "Attention between 'work' and 'outline': 0.03\n",
      "Attention between 'Really' and 'itself': 0.03\n",
      "Attention between 'model' and 'several': 0.03\n",
      "Attention between 'Evolution' and 'Self': 0.03\n",
      "Attention between 'lines' and 'outline': 0.03\n",
      "Attention between 'capable' and 'effective': 0.03\n",
      "Attention between 'Con' and 'tasks': 0.03\n",
      "Attention between 'been' and 'numerous': 0.03\n",
      "Attention between 'Evaluation' and 'Qing': 0.03\n",
      "Attention between 'Work' and 'framework': 0.03\n",
      "Attention between 'research' and 'We': 0.03\n",
      "Attention between 'signals' and 'been': 0.03\n",
      "Attention between 'itself' and 'Hour': 0.03\n",
      "Attention between 'model' and 'Late': 0.03\n",
      "Attention between 'del' and 'model': 0.03\n",
      "Attention between 'Ron' and 'Ron': 0.03\n",
      "Attention between 'tasks' and 'We': 0.03\n",
      "Attention between 'examining' and 'surveys': 0.03\n",
      "Attention between 'been' and 'outline': 0.03\n",
      "Attention between 'Hour' and 'critical': 0.03\n",
      "Attention between 'been' and 'these': 0.03\n",
      "Attention between 'several' and 'critical': 0.03\n",
      "Attention between 'Co' and 'studies': 0.03\n",
      "Attention between 'hypothesis' and 'several': 0.03\n",
      "Attention between 'numerous' and 'We': 0.03\n",
      "Attention between 'del' and 'concern': 0.03\n",
      "Attention between 'or' and 'Para': 0.03\n",
      "Attention between 'concern' and 'numerous': 0.03\n",
      "Attention between 'signals' and 'Late': 0.02\n",
      "Attention between 'common' and 'They': 0.02\n",
      "Attention between 'We' and 'propose': 0.02\n",
      "Attention between 'either' and 'We': 0.02\n",
      "Attention between '[CLS]' and 'Almost': 0.02\n",
      "Attention between 'Evolution' and 'studies': 0.02\n",
      "Attention between 'several' and 'evaluation': 0.02\n",
      "Attention between 'employed' and 'tasks': 0.02\n",
      "Attention between 'hypothesis' and 'critical': 0.02\n",
      "Attention between 'We' and 'propose': 0.02\n",
      "Attention between 'tasks' and 'Ex': 0.02\n",
      "Attention between 'captures' and 'former': 0.02\n",
      "Attention between 'Internal' and 'captures': 0.02\n",
      "Attention between 'outline' and 'Reason': 0.02\n",
      "Attention between 'we' and 'We': 0.02\n",
      "Attention between 'Para' and 'itself': 0.02\n",
      "Attention between 'systematically' and 'concern': 0.02\n",
      "Attention between 'enhance' and 'numerous': 0.02\n",
      "Attention between 'Para' and 'research': 0.02\n",
      "Attention between 'sum' and 'been': 0.02\n",
      "Attention between 'Xu' and 'Xu': 0.02\n",
      "Attention between 'sum' and 'future': 0.02\n",
      "Attention between 'or' and 'del': 0.02\n",
      "Attention between 'propose' and 'Work': 0.02\n",
      "Attention between 'employed' and 'future': 0.02\n",
      "Attention between 'Late' and 'critical': 0.02\n",
      "Attention between 'Does' and 'itself': 0.02\n",
      "Attention between 'Con' and 'critical': 0.02\n",
      "Attention between 'lines' and 'Does': 0.02\n",
      "Attention between 'Para' and 'studies': 0.02\n",
      "Attention between 'been' and 'del': 0.02\n",
      "Attention between 'including' and 'Para': 0.02\n",
      "Attention between 'hypothesis' and 'We': 0.02\n",
      "Attention between 'generate' and 'or': 0.02\n",
      "Attention between 'promising' and 'numerous': 0.02\n",
      "Attention between 'Late' and 'research': 0.02\n",
      "Attention between 'either' and 'Ex': 0.02\n",
      "Attention between 'involving' and 'such': 0.02\n",
      "Attention between 'relevant' and 'We': 0.02\n",
      "Attention between 'Self' and 'Self': 0.02\n",
      "Attention between 'This' and 'Furthermore': 0.02\n",
      "Attention between 'sum' and 'paper': 0.02\n",
      "Attention between 'systematically' and 'We': 0.02\n",
      "Attention between 'outline' and 'Does': 0.02\n",
      "Attention between 'Does' and 'work': 0.02\n",
      "Attention between 'hypothesis' and 'outline': 0.02\n",
      "Attention between 'systematically' and 'Reason': 0.02\n",
      "Attention between 'either' and 'we': 0.02\n",
      "Attention between 'Li' and 'Li': 0.02\n",
      "Attention between 'del' and 'critical': 0.02\n",
      "Attention between 'Really' and 'concern': 0.02\n",
      "Attention between 'Late' and 'studies': 0.02\n",
      "Attention between 'perspective' and 'sum': 0.02\n",
      "Attention between 'class' and 'has': 0.02\n",
      "Attention between 'Con' and 'concern': 0.02\n",
      "Attention between 'model' and 'critical': 0.02\n",
      "Attention between 'propose' and 'Reason': 0.02\n",
      "Attention between 'methods' and 'outline': 0.02\n",
      "Attention between 'de' and 'layer': 0.02\n",
      "Attention between 'introduce' and 'framework': 0.02\n",
      "Attention between 'effective' and 'theoretical': 0.02\n",
      "Attention between 'has' and 'Almost': 0.02\n",
      "Attention between 'either' and 'outline': 0.02\n",
      "Attention between 'directions' and 'critical': 0.02\n",
      "Attention between 'We' and 'research': 0.02\n",
      "Attention between 'concern' and 'been': 0.02\n",
      "Attention between 'been' and 'content': 0.02\n",
      "Attention between 'bench' and 'We': 0.02\n",
      "Attention between 'Para' and 'We': 0.02\n",
      "Attention between 'relevant' and 'Work': 0.02\n",
      "Attention between 'Con' and 'concern': 0.02\n",
      "Attention between 'several' and 'we': 0.02\n",
      "Attention between 'been' and 'We': 0.02\n",
      "Attention between 'We' and 'studies': 0.02\n",
      "Attention between 'Con' and 'tasks': 0.02\n",
      "Attention between 'evaluation' and 'We': 0.02\n",
      "Attention between 'bench' and 'these': 0.02\n",
      "Attention between 'AU' and '14': 0.02\n",
      "Attention between 'initiated' and 'initiated': 0.02\n",
      "Attention between 'del' and 'Ex': 0.02\n",
      "Attention between 'Para' and 'been': 0.02\n",
      "Attention between 'hypothesis' and 'been': 0.02\n",
      "Attention between 'Fe' and 'model': 0.02\n",
      "Attention between 'Qing' and 'IEEE': 0.02\n",
      "Attention between 'Con' and 'LL': 0.02\n",
      "Attention between 'We' and 'research': 0.02\n",
      "Attention between 'effective' and 'yet': 0.02\n",
      "Attention between 'response' and 'critical': 0.02\n",
      "Attention between 'propose' and 'numerous': 0.02\n",
      "Attention between 'bench' and 'We': 0.02\n",
      "Attention between 'Ex' and 'critical': 0.02\n",
      "Attention between 'itself' and 'future': 0.02\n",
      "Attention between 'Evolution' and 'research': 0.02\n",
      "Attention between 'or' and 'We': 0.02\n",
      "Attention between 'Self' and 'captures': 0.02\n",
      "Attention between 'AU' and 'Li': 0.02\n",
      "Attention between 'Hour' and 'Late': 0.02\n",
      "Attention between 'We' and 'itself': 0.02\n",
      "Attention between 'involving' and 'common': 0.02\n",
      "Attention between 'Reason' and 'studies': 0.02\n",
      "Attention between 'We' and 'studies': 0.02\n",
      "Attention between 'assess' and 'assess': 0.02\n",
      "Attention between 'Ex' and 'Self': 0.02\n",
      "Attention between 'either' and 'future': 0.02\n",
      "Attention between 'we' and 'critical': 0.02\n",
      "Attention between 'critical' and 'We': 0.02\n",
      "Attention between 'systematically' and 'relevant': 0.02\n",
      "Attention between 'enhance' and 'these': 0.02\n",
      "Attention between 'methods' and 'Does': 0.02\n",
      "Attention between 'these' and 'these': 0.02\n",
      "Attention between 'prefix' and 'prefix': 0.02\n",
      "Attention between 'Does' and 'research': 0.02\n",
      "Attention between 'these' and 'Self': 0.02\n",
      "Attention between 'or' and 'we': 0.02\n",
      "Attention between 'either' and 'concern': 0.02\n",
      "Attention between 'Does' and 'studies': 0.02\n",
      "Attention between 'viewpoint' and 'We': 0.02\n",
      "Attention between 'been' and 'We': 0.02\n",
      "Attention between 'these' and 'Nonetheless': 0.02\n",
      "Attention between 'framework' and 'outline': 0.02\n",
      "Attention between 'propose' and 'these': 0.02\n",
      "Attention between 'model' and 'critical': 0.02\n",
      "Attention between 'Ex' and 'itself': 0.02\n",
      "Attention between 'future' and 'numerous': 0.02\n",
      "Attention between 'Ex' and 'Ex': 0.02\n",
      "Attention between 'promising' and 'these': 0.02\n",
      "Attention between 'have' and 'Liang': 0.02\n",
      "Attention between 'Self' and 'such': 0.02\n",
      "Attention between 'We' and 'research': 0.02\n",
      "Attention between 'common' and 'initiated': 0.02\n",
      "Attention between 'We' and 'Work': 0.02\n",
      "Attention between 'systematically' and 'research': 0.02\n",
      "Attention between 'systematically' and 'Late': 0.02\n",
      "Attention between 'systematically' and 'We': 0.02\n",
      "Attention between 'we' and 'we': 0.02\n",
      "Attention between 'Co' and 'research': 0.02\n",
      "Attention between 'numerous' and 'we': 0.02\n",
      "Attention between 'numerous' and 'Para': 0.02\n",
      "Attention between 'class' and 'future': 0.02\n",
      "Attention between 'hypothesis' and 'We': 0.02\n",
      "Attention between 'been' and 'Self': 0.02\n",
      "Attention between 'late' and 'late': 0.02\n",
      "Attention between 'Reason' and 'research': 0.02\n",
      "Attention between 'Con' and 'critical': 0.02\n",
      "Attention between 'Fe' and 'studies': 0.02\n",
      "Attention between 'itself' and 'Reason': 0.02\n",
      "Attention between 'systematically' and 'future': 0.02\n",
      "Attention between 'studies' and 'Self': 0.02\n",
      "Attention between 'Almost' and 'work': 0.02\n",
      "Attention between 'concern' and 'these': 0.02\n",
      "Attention between 'critical' and 'Late': 0.02\n",
      "Attention between 'directions' and 'Really': 0.02\n",
      "Attention between 'systematically' and 'studies': 0.02\n",
      "Attention between 'Work' and 'has': 0.02\n",
      "Attention between 'relevant' and 'outline': 0.02\n",
      "Attention between 'model' and 'several': 0.02\n",
      "Attention between 'viewpoint' and 'We': 0.02\n",
      "Attention between 'either' and 'viewpoint': 0.02\n",
      "Attention between 'framework' and 'Large': 0.02\n",
      "Attention between 'outline' and 'been': 0.02\n",
      "Attention between 'directions' and 'future': 0.02\n",
      "Attention between 'bench' and 'Ex': 0.02\n",
      "Attention between 'studies' and 'Self': 0.02\n",
      "Attention between 'phenomena' and 'explanations': 0.02\n",
      "Attention between 'Reason' and 'itself': 0.02\n",
      "Attention between 'has' and 'del': 0.02\n",
      "Attention between 'critical' and 'relevant': 0.02\n",
      "Attention between 'model' and 'We': 0.02\n",
      "Attention between 'promising' and 'concern': 0.02\n",
      "Attention between 'relevant' and 'del': 0.02\n",
      "Attention between 'work' and 'Does': 0.02\n",
      "Attention between 'termed' and 'paper': 0.02\n",
      "Attention between 'future' and 'Self': 0.02\n",
      "Attention between 'signals' and 'outline': 0.02\n",
      "Attention between 'critical' and 'We': 0.02\n",
      "Attention between 'hypothesis' and 'has': 0.02\n",
      "Attention between 'stream' and 'we': 0.02\n",
      "Attention between 'systematically' and 'Work': 0.02\n",
      "Attention between 'This' and 'Work': 0.02\n",
      "Attention between 'several' and 'class': 0.02\n",
      "Attention between 'these' and 'been': 0.02\n",
      "Attention between 'phenomena' and 'offers': 0.02\n",
      "Attention between 'has' and 'research': 0.02\n",
      "Attention between 'numerous' and 'concern': 0.02\n",
      "Attention between 'Self' and 'Self': 0.02\n",
      "Attention between 'model' and 'Late': 0.02\n",
      "Attention between 'We' and 'studies': 0.02\n",
      "Attention between 'We' and 'itself': 0.02\n",
      "Attention between 'Evolution' and 'model': 0.02\n",
      "Attention between 'several' and 'future': 0.02\n",
      "Attention between 'including' and 'outline': 0.02\n",
      "Attention between 'has' and 'studies': 0.02\n",
      "Attention between 'Evolution' and 'itself': 0.02\n",
      "Attention between 'enhance' and 'been': 0.02\n",
      "Attention between 'Con' and 'future': 0.02\n",
      "Attention between 'framework' and 'research': 0.02\n",
      "Attention between 'propose' and 'future': 0.02\n",
      "Attention between 'Really' and 'critical': 0.02\n",
      "Attention between 'We' and 'hypothesis': 0.02\n",
      "Attention between 'been' and 'Work': 0.02\n",
      "Attention between 'Late' and 'itself': 0.02\n",
      "Attention between 'Co' and 'concern': 0.02\n",
      "Attention between 'propose' and 'research': 0.02\n",
      "Attention between 'research' and 'outline': 0.02\n",
      "Attention between 'framework' and 'itself': 0.02\n",
      "Attention between 'We' and 'itself': 0.02\n",
      "Attention between 'these' and 'future': 0.02\n",
      "Attention between 'Member' and 'Member': 0.02\n",
      "Attention between 'future' and 'tasks': 0.02\n",
      "Attention between 'sum' and 'we': 0.02\n",
      "Attention between 'methods' and 'Really': 0.02\n",
      "Attention between 'theoretical' and 'Large': 0.02\n",
      "Attention between 'often' and 'often': 0.02\n",
      "Attention between '202' and 'Jul': 0.02\n",
      "Attention between 'including' and 'evaluation': 0.02\n",
      "Attention between 'bench' and 'viewpoint': 0.02\n",
      "Attention between 'future' and 'evaluation': 0.02\n",
      "Attention between 'has' and 'Does': 0.02\n",
      "Attention between 'we' and 'numerous': 0.02\n",
      "Attention between 'unified' and 'these': 0.02\n",
      "Attention between 'enhance' and 'Almost': 0.02\n",
      "Attention between 'class' and 'Really': 0.02\n",
      "Attention between 'signals' and 'several': 0.02\n",
      "Attention between 'viewpoint' and 'Reason': 0.02\n",
      "Attention between 'Evaluation' and 'Self': 0.02\n",
      "Attention between 'studies' and 'future': 0.02\n",
      "Attention between 'Internal' and 'studies': 0.02\n",
      "Attention between 'We' and 'Furthermore': 0.02\n",
      "Attention between 'critical' and 'Really': 0.02\n",
      "Attention between 'we' and 'these': 0.02\n",
      "Attention between 'Furthermore' and 'these': 0.02\n",
      "Attention between 'del' and 'tasks': 0.02\n",
      "Attention between 'Con' and 'viewpoint': 0.02\n",
      "Attention between 'Con' and 'viewpoint': 0.02\n",
      "Attention between 'Internal' and 'Self': 0.02\n",
      "Attention between 'itself' and 'Really': 0.02\n",
      "Attention between 'Self' and 'We': 0.02\n",
      "Attention between 'methods' and 'Self': 0.02\n",
      "Attention between 'critical' and 'we': 0.02\n",
      "Attention between 'tasks' and 'sum': 0.02\n",
      "Attention between 'Furthermore' and 'Self': 0.02\n",
      "Attention between 'methods' and 'critical': 0.02\n",
      "Attention between 'studies' and 'future': 0.02\n",
      "Attention between 'systematically' and 'critical': 0.02\n",
      "Attention between 'Fe' and 'Does': 0.02\n",
      "Attention between 'framework' and 'studies': 0.02\n",
      "Attention between 'Para' and 'work': 0.02\n",
      "Attention between 'Para' and 'critical': 0.02\n",
      "Attention between 'work' and 'been': 0.02\n",
      "Attention between 'We' and 'We': 0.02\n",
      "Attention between 'Co' and 'Late': 0.02\n",
      "Attention between 'Evolution' and 'Late': 0.02\n",
      "Attention between 'surveys' and 'existing': 0.02\n",
      "Attention between 'framework' and 'Really': 0.02\n",
      "Attention between 'We' and 'Work': 0.02\n",
      "Attention between 'been' and 'Liang': 0.02\n",
      "Attention between '[CLS]' and 'bench': 0.02\n",
      "Attention between 'def' and 'often': 0.02\n",
      "Attention between 'research' and 'numerous': 0.02\n",
      "Attention between 'research' and 'We': 0.02\n",
      "Attention between 'sum' and 'theoretical': 0.02\n",
      "Attention between 'itself' and 'outline': 0.02\n",
      "Attention between 'has' and 'Reason': 0.02\n",
      "Attention between 'research' and 'has': 0.02\n",
      "Attention between 'latter' and 'signals': 0.02\n",
      "Attention between 'bench' and 'been': 0.02\n",
      "Attention between 'Fe' and 'research': 0.02\n",
      "Attention between 'mining' and 'framework': 0.02\n",
      "Attention between 'critical' and 'Does': 0.02\n",
      "Attention between 'We' and 'We': 0.02\n",
      "Attention between 'Furthermore' and 'class': 0.02\n",
      "Attention between 'been' and 'Reason': 0.02\n",
      "Attention between '145' and '145': 0.02\n",
      "Attention between 'class' and 'Reason': 0.02\n",
      "Attention between 'Senior' and '202': 0.02\n",
      "Attention between 'signals' and 'Ex': 0.02\n",
      "Attention between 'either' and 'We': 0.02\n",
      "Attention between 'Con' and 'been': 0.02\n",
      "Attention between 'outline' and 'viewpoint': 0.02\n",
      "Attention between 'captures' and 'Li': 0.02\n",
      "Attention between 'Con' and 'work': 0.02\n",
      "Attention between 'systematically' and 'numerous': 0.02\n",
      "Attention between 'relevant' and 'We': 0.02\n",
      "Attention between 'future' and 'these': 0.02\n",
      "Attention between 'these' and 'tasks': 0.02\n",
      "Attention between 'Almost' and 'Self': 0.02\n",
      "Attention between 'Work' and 'Self': 0.02\n",
      "Attention between 'Li' and 'Li': 0.02\n",
      "Attention between 'Furthermore' and 'numerous': 0.02\n",
      "Attention between 'these' and 'evaluation': 0.02\n",
      "Attention between 'hypothesis' and 'Hour': 0.02\n",
      "Attention between 'future' and 'concern': 0.02\n",
      "Attention between 'bench' and 'del': 0.02\n",
      "Attention between 'presence' and 'presence': 0.02\n",
      "Attention between 'either' and 'We': 0.02\n",
      "Attention between 'including' and 'del': 0.02\n",
      "Attention between 'sum' and 'tasks': 0.02\n",
      "Attention between 'generate' and 'accurately': 0.02\n",
      "Attention between 'bench' and 'Really': 0.02\n",
      "Attention between 'tasks' and 'We': 0.02\n",
      "Attention between 'studies' and 'Hour': 0.02\n",
      "Attention between 'we' and 'this': 0.02\n",
      "Attention between 'content' and 'late': 0.02\n",
      "Attention between 'We' and 'We': 0.02\n",
      "Attention between 'Co' and 'viewpoint': 0.02\n",
      "Attention between 'evaluation' and 'tasks': 0.02\n",
      "Attention between 'mit' and 'issues': 0.02\n",
      "Attention between 'reasoning' and 'def': 0.02\n",
      "Attention between 'either' and 'Reason': 0.02\n",
      "Attention between 'Evolution' and 'tasks': 0.02\n",
      "Attention between 'has' and 'Really': 0.02\n",
      "Attention between 'we' and 'Para': 0.02\n",
      "Attention between 'methods' and 'Reason': 0.02\n",
      "Attention between 'outline' and 'numerous': 0.02\n",
      "Attention between 'tasks' and 'Reason': 0.02\n",
      "Attention between 'propose' and 'studies': 0.02\n",
      "Attention between 'predominantly' and 'surveys': 0.02\n",
      "Attention between '[CLS]' and 'work': 0.02\n",
      "Attention between 'model' and 'We': 0.02\n",
      "Attention between 'We' and 'critical': 0.02\n",
      "Attention between 'enhance' and 'viewpoint': 0.02\n",
      "Attention between 'studies' and 'Hour': 0.02\n",
      "Attention between 'relevant' and 'been': 0.01\n",
      "Attention between 'propose' and 'del': 0.01\n",
      "Attention between 'future' and 'This': 0.01\n",
      "Attention between 'We' and 'Work': 0.01\n",
      "Attention between 'Self' and 'modules': 0.01\n",
      "Attention between 'either' and 'class': 0.01\n",
      "Attention between 'We' and 'concern': 0.01\n",
      "Attention between 'class' and 'Fe': 0.01\n",
      "Attention between 'model' and 'outline': 0.01\n",
      "Attention between 'Almost' and 'concern': 0.01\n",
      "Attention between 'work' and 'Reason': 0.01\n",
      "Attention between 'model' and 'been': 0.01\n",
      "Attention between 'work' and 'future': 0.01\n",
      "Attention between 'employed' and 'critical': 0.01\n",
      "Attention between 'consists' and 'signals': 0.01\n",
      "Attention between 'Self' and 'critical': 0.01\n",
      "Attention between 'Does' and 'concern': 0.01\n",
      "Attention between 'evaluation' and 'future': 0.01\n",
      "Attention between 'Really' and 'been': 0.01\n",
      "Attention between 'enhance' and 'tasks': 0.01\n",
      "Attention between 'common' and 'common': 0.01\n",
      "Attention between 'framework' and 'Large': 0.01\n",
      "Attention between 'future' and 'future': 0.01\n",
      "Attention between 'We' and 'Furthermore': 0.01\n",
      "Attention between 'bench' and 'Reason': 0.01\n",
      "Attention between 'Furthermore' and 'work': 0.01\n",
      "Attention between 'has' and 'concern': 0.01\n",
      "Attention between 'we' and 'future': 0.01\n",
      "Attention between 'Co' and 'work': 0.01\n",
      "Attention between 'Con' and 'Internal': 0.01\n",
      "Attention between 'model' and 'We': 0.01\n",
      "Attention between 'evaluation' and 'concern': 0.01\n",
      "Attention between 'lines' and 'future': 0.01\n",
      "Attention between 'propose' and 'itself': 0.01\n",
      "Attention between 'or' and 'We': 0.01\n",
      "Attention between 'signals' and 'Really': 0.01\n",
      "Attention between 'lines' and 'sum': 0.01\n",
      "Attention between '202' and 'Self': 0.01\n",
      "Attention between 'signals' and 'Reason': 0.01\n",
      "Attention between 'two' and 'consists': 0.01\n",
      "Attention between 'Evaluation' and 'mining': 0.01\n",
      "Attention between 'Co' and 'itself': 0.01\n",
      "Attention between 'work' and 'Hour': 0.01\n",
      "Attention between 'employed' and 'Ex': 0.01\n",
      "Attention between 'This' and 'concern': 0.01\n",
      "Attention between 'sum' and 'We': 0.01\n",
      "Attention between 'Internal' and 'research': 0.01\n",
      "Attention between 'bench' and 'Late': 0.01\n",
      "Attention between 'these' and 'viewpoint': 0.01\n",
      "Attention between 'we' and 'propose': 0.01\n",
      "Attention between 'class' and 'Ex': 0.01\n",
      "Attention between 'been' and 'evaluation': 0.01\n",
      "Attention between 'This' and 'itself': 0.01\n",
      "Attention between '[CLS]' and 'We': 0.01\n",
      "Attention between 'we' and 'been': 0.01\n",
      "Attention between 'We' and 'hypothesis': 0.01\n",
      "Attention between 'several' and 'concern': 0.01\n",
      "Attention between 'tasks' and 'has': 0.01\n",
      "Attention between 'Evolution' and 'Furthermore': 0.01\n",
      "Attention between 'outline' and 'Late': 0.01\n",
      "Attention between 'or' and 'critical': 0.01\n",
      "Attention between 'Fe' and 'Ex': 0.01\n",
      "Attention between 'model' and 'We': 0.01\n",
      "Attention between 'Almost' and 'Furthermore': 0.01\n",
      "Attention between 'model' and 'Reason': 0.01\n",
      "Attention between 'Self' and 'We': 0.01\n",
      "Attention between 'propose' and 'We': 0.01\n",
      "Attention between 'itself' and 'concern': 0.01\n",
      "Attention between 'Co' and 'critical': 0.01\n",
      "Attention between 'tasks' and 'Self': 0.01\n",
      "Attention between 'sampling' and 'based': 0.01\n",
      "Attention between 'We' and 'We': 0.01\n",
      "Attention between 'del' and 'viewpoint': 0.01\n",
      "Attention between 'concern' and 'concern': 0.01\n",
      "Attention between 'Evolution' and 'concern': 0.01\n",
      "Attention between 'LA' and '202': 0.01\n",
      "Attention between 'model' and 'been': 0.01\n",
      "Attention between 'Really' and 'We': 0.01\n",
      "Attention between 'Really' and 'hypothesis': 0.01\n",
      "Attention between 'methods' and 'Fe': 0.01\n",
      "Attention between 'work' and 'numerous': 0.01\n",
      "Attention between 'Jul' and 'LL': 0.01\n",
      "Attention between 'relevant' and 'we': 0.01\n",
      "Attention between 'del' and 'We': 0.01\n",
      "Attention between 'has' and 'itself': 0.01\n",
      "Attention between 'studies' and 'been': 0.01\n",
      "Attention between 'signals' and 'del': 0.01\n",
      "Attention between 'We' and 'work': 0.01\n",
      "Attention between 'Reason' and 'Self': 0.01\n",
      "Attention between 'evaluation' and 'Really': 0.01\n",
      "Attention between 'del' and 'been': 0.01\n",
      "Attention between 'Con' and 'work': 0.01\n",
      "Attention between 'directions' and 'Reason': 0.01\n",
      "Attention between 'lines' and 'Really': 0.01\n",
      "Attention between 'Self' and 'Self': 0.01\n",
      "Attention between 'these' and 'We': 0.01\n",
      "Attention between 'Almost' and 'tasks': 0.01\n",
      "Attention between 'propose' and 'We': 0.01\n",
      "Attention between 'del' and 'signals': 0.01\n",
      "Attention between 'generate' and 'expected': 0.01\n",
      "Attention between 'Con' and 'future': 0.01\n",
      "Attention between 'future' and 'methods': 0.01\n",
      "Attention between 'tasks' and 'future': 0.01\n",
      "Attention between 'itself' and 'Does': 0.01\n",
      "Attention between 'viewpoint' and 'outline': 0.01\n",
      "Attention between 'relevant' and 'Late': 0.01\n",
      "Attention between 'signals' and 'future': 0.01\n",
      "Attention between 'We' and 'Furthermore': 0.01\n",
      "Attention between 'Con' and 'been': 0.01\n",
      "Attention between 'these' and 'critical': 0.01\n",
      "Attention between 'either' and 'Furthermore': 0.01\n",
      "Attention between 'tasks' and 'del': 0.01\n",
      "Attention between 'yet' and 'We': 0.01\n",
      "Attention between 'including' and 'Ex': 0.01\n",
      "Attention between 'critical' and 'Reason': 0.01\n",
      "Attention between 'been' and 'research': 0.01\n",
      "Attention between 'promising' and 'has': 0.01\n",
      "Attention between 'Ex' and 'future': 0.01\n",
      "Attention between 'response' and 'tasks': 0.01\n",
      "Attention between 'effective' and 'stream': 0.01\n",
      "Attention between 'future' and 'Furthermore': 0.01\n",
      "Attention between 'promising' and 'evaluation': 0.01\n",
      "Attention between 'evaluation' and 'We': 0.01\n",
      "Attention between 'have' and 'content': 0.01\n",
      "Attention between 'several' and 'promising': 0.01\n",
      "Attention between 'systematically' and 'itself': 0.01\n",
      "Attention between 'response' and 'Does': 0.01\n",
      "Attention between 'Fe' and 'concern': 0.01\n",
      "Attention between 'framework' and 'Does': 0.01\n",
      "Attention between 'either' and 'tasks': 0.01\n",
      "Attention between 'research' and 'future': 0.01\n",
      "Attention between 'research' and 'Fe': 0.01\n",
      "Attention between 'class' and 'framework': 0.01\n",
      "Attention between 'tasks' and 'been': 0.01\n",
      "Attention between 'Internal' and 'We': 0.01\n",
      "Attention between 'concern' and 'We': 0.01\n",
      "Attention between 'Self' and 'been': 0.01\n",
      "Attention between 'named' and 'theoretical': 0.01\n",
      "Attention between 'This' and 'studies': 0.01\n",
      "Attention between 'numerous' and 'del': 0.01\n",
      "Attention between 'This' and 'research': 0.01\n",
      "Attention between 'Internal' and 'concern': 0.01\n",
      "Attention between 'these' and 'class': 0.01\n",
      "Attention between 'hypothesis' and 'Reason': 0.01\n",
      "Attention between 'or' and 'class': 0.01\n",
      "Attention between 'del' and 'research': 0.01\n",
      "Attention between 'hall' and 'critical': 0.01\n",
      "Attention between 'We' and 'concern': 0.01\n",
      "Attention between 'propose' and 'evaluation': 0.01\n",
      "Attention between 'including' and 'we': 0.01\n",
      "Attention between 'hypothesis' and 'framework': 0.01\n",
      "Attention between '145' and 'Xu': 0.01\n",
      "Attention between 'promising' and 'research': 0.01\n",
      "Attention between 'either' and 'Hour': 0.01\n",
      "Attention between 'effective' and 'we': 0.01\n",
      "Attention between 'itself' and 'sum': 0.01\n",
      "Attention between 'class' and 'outline': 0.01\n",
      "Attention between 'Really' and 'future': 0.01\n",
      "Attention between 'studies' and 'has': 0.01\n",
      "Attention between 'del' and 'numerous': 0.01\n",
      "Attention between 'either' and 'Self': 0.01\n",
      "Attention between 'Really' and 'Self': 0.01\n",
      "Attention between 'Hu' and 'Fe': 0.01\n",
      "Attention between 'several' and 'We': 0.01\n",
      "Attention between 'research' and 'Reason': 0.01\n",
      "Attention between 'Really' and 'Furthermore': 0.01\n",
      "Attention between 'systematically' and 'Really': 0.01\n",
      "Attention between 'Hour' and 'Self': 0.01\n",
      "Attention between 'sum' and 'this': 0.01\n",
      "Attention between 'been' and 'concern': 0.01\n",
      "Attention between 'Late' and 'been': 0.01\n",
      "Attention between 'outline' and 'Almost': 0.01\n",
      "Attention between 'Hour' and 'Does': 0.01\n",
      "Attention between 'hypothesis' and 'itself': 0.01\n",
      "Attention between 'Self' and 'Li': 0.01\n",
      "Attention between 'has' and 'Para': 0.01\n",
      "Attention between 'hypothesis' and 'research': 0.01\n",
      "Attention between 'enhance' and 'research': 0.01\n",
      "Attention between 'del' and 'studies': 0.01\n",
      "Attention between 'research' and 'framework': 0.01\n",
      "Attention between 'We' and 'We': 0.01\n",
      "Attention between 'relevant' and 'Reason': 0.01\n",
      "Attention between 'research' and 'these': 0.01\n",
      "Attention between 'systematically' and 'model': 0.01\n",
      "Attention between 'Fe' and 'tasks': 0.01\n",
      "Attention between 'been' and 'future': 0.01\n",
      "Attention between 'we' and 'Reason': 0.01\n",
      "Attention between 'numerous' and 'Ex': 0.01\n",
      "Attention between 'hypothesis' and 'evaluation': 0.01\n",
      "Attention between 'initiated' and 'been': 0.01\n",
      "Attention between 'Furthermore' and 'Almost': 0.01\n",
      "Attention between 'employed' and 'research': 0.01\n",
      "Attention between 'has' and 'numerous': 0.01\n",
      "Attention between 'future' and 'Para': 0.01\n",
      "Attention between 'critical' and 'del': 0.01\n",
      "Attention between 'model' and 'has': 0.01\n",
      "Attention between '19' and 'Models': 0.01\n",
      "Attention between 'or' and 'research': 0.01\n",
      "Attention between 'tasks' and 'Fe': 0.01\n",
      "Attention between 'work' and 'these': 0.01\n",
      "Attention between 'We' and 'critical': 0.01\n",
      "Attention between 'evaluation' and 'Self': 0.01\n",
      "Attention between 'Co' and 'tasks': 0.01\n",
      "Attention between 'these' and 'Furthermore': 0.01\n",
      "Attention between 'Con' and 'Work': 0.01\n",
      "Attention between 'lack' and 'lack': 0.01\n",
      "Attention between 'we' and 'class': 0.01\n",
      "Attention between 'Fe' and 'including': 0.01\n",
      "Attention between 'relevant' and 'Para': 0.01\n",
      "Attention between 'model' and 'Reason': 0.01\n",
      "Attention between 'enhance' and 'Work': 0.01\n",
      "Attention between 'outline' and 'research': 0.01\n",
      "Attention between 'employed' and 'concern': 0.01\n",
      "Attention between 'Con' and 'We': 0.01\n",
      "Attention between 'studies' and 'several': 0.01\n",
      "Attention between 'promising' and 'Furthermore': 0.01\n",
      "Attention between 'viewpoint' and 'future': 0.01\n",
      "Attention between 'effective' and 'introduce': 0.01\n",
      "Attention between 'class' and 'several': 0.01\n",
      "Attention between 'Member' and 'Han': 0.01\n",
      "Attention between 'Self' and '202': 0.01\n",
      "Attention between 'Con' and 'mining': 0.01\n",
      "Attention between 'Con' and 'We': 0.01\n",
      "Attention between 'We' and 'concern': 0.01\n",
      "Attention between 'model' and 'response': 0.01\n",
      "Attention between 'systematically' and 'Furthermore': 0.01\n",
      "Attention between 'studies' and 'several': 0.01\n",
      "Attention between 'research' and 'viewpoint': 0.01\n",
      "Attention between 'Hour' and 'including': 0.01\n",
      "Attention between 'employed' and 'Late': 0.01\n",
      "Attention between 'either' and 'del': 0.01\n",
      "Attention between 'been' and 'we': 0.01\n",
      "Attention between 'We' and 'We': 0.01\n",
      "Attention between 'studies' and 'Late': 0.01\n",
      "Attention between 'future' and 'relevant': 0.01\n",
      "Attention between 'while' and 'language': 0.01\n",
      "Attention between 'address' and 'these': 0.01\n",
      "Attention between 'several' and 'Ex': 0.01\n",
      "Attention between 'sum' and 'numerous': 0.01\n",
      "Attention between 'been' and 'bench': 0.01\n",
      "Attention between 'framework' and 'theoretical': 0.01\n",
      "Attention between 'hypothesis' and 'We': 0.01\n",
      "Attention between 'evaluation' and 'Late': 0.01\n",
      "Attention between 'Almost' and 'future': 0.01\n",
      "Attention between 'methods' and 'studies': 0.01\n",
      "Attention between 'propose' and 'concern': 0.01\n",
      "Attention between 'Evolution' and 'Really': 0.01\n",
      "Attention between 'been' and 'Self': 0.01\n",
      "Attention between 'enhance' and 'concern': 0.01\n",
      "Attention between 'We' and 'work': 0.01\n",
      "Attention between 'Para' and 'model': 0.01\n",
      "Attention between 'del' and 'including': 0.01\n",
      "Attention between 'been' and 'Self': 0.01\n",
      "Attention between 'these' and 'these': 0.01\n",
      "Attention between 'Internal' and 'itself': 0.01\n",
      "Attention between 'presence' and 'tasks': 0.01\n",
      "Attention between 'hypothesis' and 'This': 0.01\n",
      "Attention between 'evaluation' and 'has': 0.01\n",
      "Attention between 'critical' and 'class': 0.01\n",
      "Attention between 'Work' and 'model': 0.01\n",
      "Attention between 'employed' and 'Self': 0.01\n",
      "Attention between 'including' and 'Hour': 0.01\n",
      "Attention between 'Internal' and 'work': 0.01\n",
      "Attention between 'model' and 'has': 0.01\n",
      "Attention between 'Work' and 'tasks': 0.01\n",
      "Attention between 'concern' and 'Really': 0.01\n",
      "Attention between 'model' and 'Hour': 0.01\n",
      "Attention between 'response' and 'several': 0.01\n",
      "Attention between 'directions' and 'Does': 0.01\n",
      "Attention between 'Ex' and 'model': 0.01\n",
      "Attention between 'tasks' and 'Hour': 0.01\n",
      "Attention between 'viewpoint' and 'numerous': 0.01\n",
      "Attention between 'studies' and 'these': 0.01\n",
      "Attention between 'Con' and 'Furthermore': 0.01\n",
      "Attention between 'Co' and 'Furthermore': 0.01\n",
      "Attention between 'model' and 'outline': 0.01\n",
      "Attention between 'methods' and 'model': 0.01\n",
      "Attention between 'Hour' and 'model': 0.01\n",
      "Attention between 'sum' and 'termed': 0.01\n",
      "Attention between 'Work' and 'studies': 0.01\n",
      "Attention between 'studies' and 'We': 0.01\n",
      "Attention between 'several' and 'Self': 0.01\n",
      "Attention between 'efforts' and 'these': 0.01\n",
      "Attention between 'Re' and 'Self': 0.01\n",
      "Attention between 'has' and 'these': 0.01\n",
      "Attention between 'studies' and 'Fe': 0.01\n",
      "Attention between 'work' and 'We': 0.01\n",
      "Attention between 'methods' and 'research': 0.01\n",
      "Attention between 'studies' and 'We': 0.01\n",
      "Attention between 'studies' and 'been': 0.01\n",
      "Attention between 'directions' and 'relevant': 0.01\n",
      "Attention between 'framework' and 'theoretical': 0.01\n",
      "Attention between 'introduce' and 'we': 0.01\n",
      "Attention between 'Con' and 'response': 0.01\n",
      "Attention between 'either' and 'has': 0.01\n",
      "Attention between '14' and 'Internal': 0.01\n",
      "Attention between 'systematically' and 'these': 0.01\n",
      "Attention between 'relevant' and 'This': 0.01\n",
      "Attention between 'Para' and 'Ex': 0.01\n",
      "Attention between '240' and '19': 0.01\n",
      "Attention between 'Ex' and 'response': 0.01\n",
      "Attention between 'we' and 'enhance': 0.01\n",
      "Attention between 'methods' and 'has': 0.01\n",
      "Attention between 'Evolution' and 'framework': 0.01\n",
      "Attention between '14' and 'mining': 0.01\n",
      "Attention between 'research' and 'concern': 0.01\n",
      "Attention between 'generate' and 'often': 0.01\n",
      "Attention between 'Re' and 'Re': 0.01\n",
      "Attention between 'hypothesis' and 'studies': 0.01\n",
      "Attention between 'model' and 'Fe': 0.01\n",
      "Attention between 'IEEE' and 'Han': 0.01\n",
      "Attention between 'viewpoint' and 'Self': 0.01\n",
      "Attention between 'relevant' and 'promising': 0.01\n",
      "Attention between 'lines' and 'del': 0.01\n",
      "Attention between 'employed' and 'numerous': 0.01\n",
      "Attention between 'studies' and 'model': 0.01\n",
      "Attention between 'exhibit' and 'often': 0.01\n",
      "Attention between 'either' and 'critical': 0.01\n",
      "Attention between 'Fe' and 'itself': 0.01\n",
      "Attention between '[CLS]' and 'del': 0.01\n",
      "Attention between 'studies' and 'Reason': 0.01\n",
      "Attention between 'been' and 'studies': 0.01\n",
      "Attention between 'surveys' and 'surveys': 0.01\n",
      "Attention between 'studies' and 'has': 0.01\n",
      "Attention between 'sum' and 'IEEE': 0.01\n",
      "Attention between 'several' and 'del': 0.01\n",
      "Attention between 'lines' and 'Fe': 0.01\n",
      "Attention between 'this' and 'this': 0.01\n",
      "Attention between 'Furthermore' and 'methods': 0.01\n",
      "Attention between 'Con' and 'model': 0.01\n",
      "Attention between 'outline' and 'these': 0.01\n",
      "Attention between 'We' and 'critical': 0.01\n",
      "Attention between 'viewpoint' and 'these': 0.01\n",
      "Attention between 'consists' and 'theoretical': 0.01\n",
      "Attention between 'Does' and 'Hour': 0.01\n",
      "Attention between 'Internal' and 'Ex': 0.01\n",
      "Attention between 'Evolution' and 'has': 0.01\n",
      "Attention between 'research' and 'Self': 0.01\n",
      "Attention between 'Para' and 'Self': 0.01\n",
      "Attention between 'several' and 'Fe': 0.01\n",
      "Attention between 'work' and 'Late': 0.01\n",
      "Attention between 'signals' and 'captures': 0.01\n",
      "Attention between 'Late' and 'Late': 0.01\n",
      "Attention between 'Self' and 'LL': 0.01\n",
      "Attention between 'studies' and 'Reason': 0.01\n",
      "Attention between 'studies' and 'Fe': 0.01\n",
      "Attention between 'Work' and 'concern': 0.01\n",
      "Attention between 'sum' and 'concern': 0.01\n",
      "Attention between 'has' and 'We': 0.01\n",
      "Attention between 'future' and 'research': 0.01\n",
      "Attention between 'class' and 'We': 0.01\n",
      "Attention between 'research' and 'response': 0.01\n",
      "Attention between 'studies' and 'We': 0.01\n",
      "Attention between 'Work' and 'critical': 0.01\n",
      "Attention between 'or' and 'Ex': 0.01\n",
      "Attention between 'Internal' and 'future': 0.01\n",
      "Attention between 'framework' and 'We': 0.01\n",
      "Attention between 'studies' and 'Does': 0.01\n",
      "Attention between 'outline' and 'studies': 0.01\n",
      "Attention between 'employed' and 'Work': 0.01\n",
      "Attention between 'Con' and 'involving': 0.01\n",
      "Attention between 'relevant' and 'research': 0.01\n",
      "Attention between 'lines' and 'several': 0.01\n",
      "Attention between 'Evolution' and 'methods': 0.01\n",
      "Attention between 'several' and 'Internal': 0.01\n",
      "Attention between 'framework' and 'critical': 0.01\n",
      "Attention between 'This' and 'work': 0.01\n",
      "Attention between 'Co' and 'relevant': 0.01\n",
      "Attention between 'been' and 'signals': 0.01\n",
      "Attention between 'including' and 'We': 0.01\n",
      "Attention between 'work' and 'We': 0.01\n",
      "Attention between 'tasks' and 'response': 0.01\n",
      "Attention between 'directions' and 'Para': 0.01\n",
      "Attention between 'Late' and 'concern': 0.01\n",
      "Attention between 'been' and 'Almost': 0.01\n",
      "Attention between 'propose' and 'Almost': 0.01\n",
      "Attention between 'or' and 'itself': 0.01\n",
      "Attention between 'Ex' and 'Furthermore': 0.01\n",
      "Attention between 'enhance' and 'del': 0.01\n",
      "Attention between 'concern' and 'Almost': 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token1</th>\n",
       "      <th>Token2</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>existing</td>\n",
       "      <td>We</td>\n",
       "      <td>0.989953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Han</td>\n",
       "      <td>We</td>\n",
       "      <td>0.989828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fe</td>\n",
       "      <td>We</td>\n",
       "      <td>0.989488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>method</td>\n",
       "      <td>We</td>\n",
       "      <td>0.982627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self</td>\n",
       "      <td>We</td>\n",
       "      <td>0.975270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>propose</td>\n",
       "      <td>Almost</td>\n",
       "      <td>0.010060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>or</td>\n",
       "      <td>itself</td>\n",
       "      <td>0.010050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>Ex</td>\n",
       "      <td>Furthermore</td>\n",
       "      <td>0.010032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>enhance</td>\n",
       "      <td>del</td>\n",
       "      <td>0.010020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>concern</td>\n",
       "      <td>Almost</td>\n",
       "      <td>0.010015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1171 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Token1       Token2     Score\n",
       "0     existing           We  0.989953\n",
       "1          Han           We  0.989828\n",
       "2           Fe           We  0.989488\n",
       "3       method           We  0.982627\n",
       "4         Self           We  0.975270\n",
       "...        ...          ...       ...\n",
       "1166   propose       Almost  0.010060\n",
       "1167        or       itself  0.010050\n",
       "1168        Ex  Furthermore  0.010032\n",
       "1169   enhance          del  0.010020\n",
       "1170   concern       Almost  0.010015\n",
       "\n",
       "[1171 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Filter and sort the attention scores above 0.5\n",
    "# Create a mask where attention values are greater than 0.5\n",
    "high_attention_mask = attention > 0.01\n",
    "\n",
    "# Apply mask to the attention matrix to filter values\n",
    "high_attention_values = attention[high_attention_mask]\n",
    "\n",
    "# Get the corresponding pairs of tokens for each high attention value\n",
    "token_pairs = [(tokens[i], tokens[j]) for i, j in zip(*np.where(high_attention_mask))]\n",
    "\n",
    "# Sort the pairs by attention values in descending order\n",
    "sorted_pairs = sorted(zip(token_pairs, high_attention_values), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a regex pattern that matches undesired tokens containing special characters\n",
    "pattern = re.compile(r'[#.\\-%$]')\n",
    "\n",
    "# Filter and sort the attention scores above 0.5, excluding pairs with special characters\n",
    "# Note: This step assumes 'sorted_pairs' has already been defined using the previous snippet\n",
    "cleaned_pairs = [\n",
    "    (pair, score) for pair, score in sorted_pairs \n",
    "    if not (pattern.search(pair[0]) or pattern.search(pair[1]))\n",
    "]\n",
    "\n",
    "# List of common English prepositions\n",
    "prepositions = set([\n",
    "    'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among',\n",
    "    'anti', 'around', 'as', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'besides',\n",
    "    'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during',\n",
    "    'except', 'excepting', 'excluding', 'following', 'for', 'from', 'in', 'inside', 'into',\n",
    "    'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'outside', 'over',\n",
    "    'past', 'per', 'plus', 'regarding', 'round', 'save', 'since', 'than', 'through', 'to',\n",
    "    'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus',\n",
    "    'via', 'with', 'within', 'without','the','is','and'\n",
    "])\n",
    "\n",
    "# Filter and sort the attention scores above 0.5, excluding pairs with special characters or prepositions\n",
    "cleaned_pairs = [\n",
    "    (pair, score) for pair, score in sorted_pairs \n",
    "    if not (pattern.search(pair[0]) or pattern.search(pair[1]) or\n",
    "            pair[0].lower() in prepositions or pair[1].lower() in prepositions or \n",
    "            len(pair[0]) == 1 or len(pair[1]) == 1)  # Check if either token is a single character)\n",
    "]\n",
    "\n",
    "# Display the cleaned, high attention token pairs with their scores\n",
    "for (token1, token2), score in cleaned_pairs:\n",
    "    print(f\"Attention between '{token1}' and '{token2}': {score:.2f}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert cleaned_pairs to a DataFrame\n",
    "data = [(pair[0], pair[1], score) for (pair, score) in cleaned_pairs]\n",
    "df = pd.DataFrame(data, columns=[\"Token1\", \"Token2\", \"Score\"])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dataframe to json file \n",
    "json_data = df.to_json(orient=\"records\", lines=True)\n",
    "\n",
    "# Save JSON to a file\n",
    "output_path = 'nvda_pairs.json'\n",
    "with open(output_path, 'w') as file:\n",
    "    file.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
